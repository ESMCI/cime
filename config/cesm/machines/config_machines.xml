<?xml version="1.0"?>

<!--

===============================================================
COMPILER and COMPILERS
===============================================================
If a machine supports multiple compilers - then
- the settings for COMPILERS should reflect the supported compilers
as a comma separated string
- the setting for COMPILER should be the default compiler
(which is one of the values in COMPILERS)

===============================================================
MPILIB and MPILIBS
===============================================================
If a machine supports only one MPILIB is supported - then
the setting for  MPILIB and MPILIBS should be blank ("")
If a machine supports multiple mpi libraries (e.g. mpich and openmpi)
- the settings for MPILIBS should reflect the supported mpi libraries
as a comma separated string

The default settings for COMPILERS and MPILIBS is blank (in config_machines.xml)

Normally variable substitutions are not made until the case scripts are run, however variables
of the form $ENV{VARIABLE_NAME} are substituted in create_newcase from the environment
variable of the same name if it exists.

===============================================================
PROJECT_REQUIRED
===============================================================
A machine may need the PROJECT xml variable to be defined either because it is
used in some paths, or because it is used to give an account number in the job
submission script. If either of these are the case, then PROJECT_REQUIRED
should be set to TRUE for the given machine.


mpirun: the mpirun command that will be used to actually launch the model.
The attributes used to choose the mpirun command are:

mpilib: can either be 'default' the name of an mpi library, or a compiler name so one can choose the mpirun
based on the mpi library in use.

the 'executable' tag must have arguments required for the chosen mpirun, as well as the executable name.

unit_testing: can be 'true' or 'false'.
This allows using a different mpirun command to launch unit tests

-->

<config_machines version="2.0">
  <machine MACH="aleph">
    <DESC>XC50 SkyLake, os is CNL, 40 pes/node, batch system is PBSPro</DESC>
    <NODENAME_REGEX>.*eth\d</NODENAME_REGEX>
    <OS>CNL</OS>
    <COMPILERS>intel,gnu,cray</COMPILERS>
    <MPILIBS>mpt,mpi-serial</MPILIBS>
    <CIME_OUTPUT_ROOT>/proj/$ENV{USER}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{DIN_LOC_ROOT}</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$DIN_LOC_ROOT</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>${CIME_OUTPUT_ROOT}/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>${CIME_OUTPUT_ROOT}/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/home/jedwards/cesm_tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY> @ pusan.ac.kr</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>40</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>40</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>aprun</executable>
      <arguments>
	<arg name="hyperthreading" default="1"> -j {{ hyperthreading }}</arg>
	<arg name="num_tasks"> -n {{ total_tasks }}</arg>
	<arg name="tasks_per_node"> -N $MAX_MPITASKS_PER_NODE</arg>
	<arg name="tasks_per_numa" > -S {{ tasks_per_numa }}</arg>
	<arg name="thread_count"> -d $ENV{OMP_NUM_THREADS}</arg>
	<arg name="env_thread_count">--mpmd-env OMP_NUM_THREADS=$OMP_NUM_THREADS</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/modules/default/init/perl.pm</init_path>
      <init_path lang="python">/opt/modules/default/init/python.py</init_path>
      <init_path lang="sh">/opt/modules/default/init/sh</init_path>
      <init_path lang="csh">/opt/modules/default/init/csh</init_path>
      <cmd_path lang="perl">/opt/modules/default/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/opt/modules/default/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="rm">craype-x86-skylake</command>
	<command name="rm">PrgEnv-pgi</command>
	<command name="rm">PrgEnv-intel</command>
	<command name="rm">PrgEnv-cray</command>
	<command name="rm">PrgEnv-gnu</command>
	<command name="rm">cray-netcdf</command>
	<command name="rm">cray-hdf5</command>
	<command name="rm">cray-parallel-netcdf</command>
	<command name="rm">papi</command>
      </modules>
      <modules compiler="intel">
	<command name="load">PrgEnv-intel</command>
	<command name="load">craype-x86-skylake</command>
	<command name="load">craype-hugepages2M</command>
	<command name="rm">perftools-base/7.0.4</command>
	<command name="load">cray-netcdf/4.6.1.3</command>
	<command name="load">cray-hdf5/1.10.2.0</command>
	<command name="load">cray-parallel-netcdf/1.11.1.1</command>
	<command name="load">papi/5.6.0.4</command>
	<command name="load">gridftp/6.0</command>
	<command name="load">cray-python/3.6.5.1</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="POSTPROCESS_PATH">/home/jedwards/workflow/CESM_postprocessing</env>
    </environment_variables>
  </machine>

  <machine MACH="arc4">
    <DESC>A port of CEM to the Leeds ARC4 machine, batch system is sge. CEMAC.</DESC>
    <NODENAME_REGEX>.*.arc.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>intelmpi,openmpi,mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>/nobackup/$ENV{USER}/cesm_sims</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{ARC4_CESM2_ROOT}/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{ARC4_CESM2_ROOT}/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>${CIME_OUTPUT_ROOT}/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>${CIME_OUTPUT_ROOT}/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>${CIMEROOT}/tools/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>16</GMAKE_J>
    <BATCH_SYSTEM>sge</BATCH_SYSTEM>
    <SUPPORTED_BY>cemac-at-leeds.ac.uk</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>40</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>40</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
	<arg name="ntasks"> -np {{ total_tasks }} </arg>
      </arguments>
    </mpirun>

    <module_system type="module" allow_error="true">
      <init_path lang="perl">/apps/Modules/default/init/perl.pm</init_path>
      <init_path lang="python">/apps/Modules/default/init/python.py</init_path>
      <init_path lang="sh">/apps/Modules/default/init/sh</init_path>
      <init_path lang="csh">/apps/Modules/default/init/csh</init_path>
      <cmd_path lang="perl">module</cmd_path>
      <cmd_path lang="python">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules compiler="intel">
	<command name="unload">openmpi</command>
	<command name="load">intel/19.0.4</command>
	<command name="load">intelmpi/2019.4.243</command>
	<command name="load">netcdf/4.6.3</command>
	<command name="load">esmf</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="archer2">
    <DESC>two CrayAMD EPYC Zen2, 128 pes/node, batch system is SLURM</DESC>
    <NODENAME_REGEX>(ln\d{2}$|nid\d{6}$)</NODENAME_REGEX>
    <OS>CNL</OS>
    <COMPILERS>gnu,cray</COMPILERS>
    <MPILIBS>mpich,mpi-serial</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{CESM_ROOT}/runs</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{CESM_ROOT}/cesm_inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>${DIN_LOC_ROOT}/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{CESM_ROOT}/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESM_ROOT}/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{CIMEROOT}/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>leeds.ac.uk</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
       <arg name="cpubind"> --distribution=block:block --hint=nomultithread</arg>
       <!--<arg name="cpubind"> -ZZ-cpu-bind=cores</arg> -->
      </arguments>
    </mpirun>
    <module_system type="module"  allow_error="true">
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/usr/share/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>

      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules compiler="gnu">
	<command name="load"> PrgEnv-gnu</command>
	<command name="load"> cray-hdf5-parallel</command>
	<command name="load"> cray-netcdf-hdf5parallel</command>
	<command name="load"> cray-parallel-netcdf</command>
	<command name="load"> cray-libsci</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="rm"> cray-netcdf-hdf5parallel</command>
	<command name="rm"> cray-hdf5-parallel</command>
	<command name="rm"> cray-parallel-netcdf</command>
	<command name="load"> cray-hdf5</command>
	<command name="load">cray-netcdf</command>
      </modules>
    </module_system>

    <environment_variables>
      <env name="PERL5LIB">/work/n02/shared/perl/5.26.2</env>
      <env name="OMP_NUM_THREADS">{{ thread_count }} </env>
      <env name="OMP_PLACES">cores </env>
      <env name="OMP_STACKSIZE">2G</env>
      <!--<env name="PATH">/work/n02/n02/csymonds/sw/conda/cesmenv2/bin:$ENV{PATH}</env>-->
    </environment_variables>

    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="athena">
    <DESC> CMCC IBM iDataPlex, os is Linux, 16 pes/node, batch system is LSFd mpich</DESC>
    <NODENAME_REGEX>.*.cluster.net</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,intel15</COMPILERS>
    <MPILIBS>mpich2</MPILIBS>
    <CIME_OUTPUT_ROOT>/work/$USER/CESM2</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/users/home/dp16116/CESM2/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$DIN_LOC_ROOT/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESMDATAROOT}/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/users/home/dp16116/CESM2/cesm2.0.1/cime/tools/cprnc/cprnc</CCSM_CPRNC>
    <PERL5LIB>/usr/lib64/perl5:/usr/share/perl5</PERL5LIB>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>lsf</BATCH_SYSTEM>
    <SUPPORTED_BY> </SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>30</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>15</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable> mpirun_Impi5 </executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python </cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
      </modules>
      <modules compiler="intel">
	<command name="load">ANACONDA2/python2.7</command>
	<command name="load">INTEL/intel_xe_2015.3.187</command>
	<command name="load">SZIP/szip-2.1_int15</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial" DEBUG="TRUE">
	<command name="load">ESMF/esmf-6.3.0rp1-intelmpi-64-g_int15</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial" DEBUG="FALSE">
	<command name="load">ESMF/esmf-6.3.0rp1-intelmpi-64-O_int15</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial" DEBUG="TRUE">
	<command name="load">ESMF/esmf-6.3.0rp1-mpiuni-64-g_int15</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial" DEBUG="FALSE">
	<command name="load">ESMF/esmf-6.3.0rp1-mpiuni-64-O_int15</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">HDF5/hdf5-1.8.15-patch1</command>
	<command name="load">NETCDF/netcdf-C_4.3.3.1-F_4.4.2_C++_4.2.1</command>
      </modules>
      <modules mpilib="!mpi-serial">
	<command name="load">HDF5/hdf5-1.8.15-patch1_parallel</command>
	<command name="load">NETCDF/netcdf-C_4.3.3.1-F_4.4.2_C++_4.2.1_parallel</command>
	<command name="load">PARALLEL_NETCDF/parallel-netcdf-1.6.1</command>
      </modules>
      <modules>
	<command name="load">CMAKE/cmake-3.3.0-rc1</command>
      </modules>
      <modules compiler="intel">
	<command name="unload">INTEL/intel_xe_2013.5.192</command>
	<command name="unload">INTEL/intel_xe_2013</command>
	<command name="unload">HDF5/hdf5-1.8.10-patch1</command>
	<command name="load">INTEL/intel_xe_2015.3.187</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="I_MPI_EXTRA_FILESYSTEM_LIST">gpfs</env>
      <env name="I_MPI_EXTRA_FILESYSTEM">on</env>
      <env name="I_MPI_PLATFORM">snb</env>
      <env name="I_MPI_HYDRA_BOOTSTRAP">lsf</env>
      <env name="I_MPI_LSF_USE_COLLECTIVE_LAUNCH">1</env>
      <env name="I_MPI_DAPL_UD">on</env>
      <env name="I_MPI_DAPL_SCALABLE_PROGRESS">on</env>
      <env name="XIOS_PATH">/users/home/models/nemo/xios-cmip6/intel_xe_2013</env>
    </environment_variables>
  </machine>

  <machine MACH="aws-hpc6a">
    <DESC>AWS HPC6a (96-core AMD) Nodes</DESC>
    <NODENAME_REGEX>regular-dy-computehpc6a-.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi</MPILIBS>
    <CIME_OUTPUT_ROOT>/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/scratch/$USER/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/scratch/$USER/inputdata/tss/CTSM_datm_forcing_data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESMDATAROOT}/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{CESMDATAROOT}/tools/cime/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>6</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <!-- have not seen any performance benefit in smt -->
    <MAX_TASKS_PER_NODE>96</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>96</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
	<arg name="num_tasks">-n {{ total_tasks }}</arg>
	<arg name="bindinfo"> --cpu-bind=verbose </arg>
	<arg name="cpu_bind">--cpu_bind=sockets --cpu_bind=verbose</arg>
	<arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="TMPDIR">/scratch/$USER</env>
      <env name="I_MPI_CC">icc</env>
      <env name="I_MPI_CXX">icpc</env>
      <env name="I_MPI_FC">ifort</env>
      <env name="I_MPI_F90">ifort</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="bluewaters">
    <DESC>ORNL XE6, os is CNL, 32 pes/node, batch system is PBS</DESC>
    <NODENAME_REGEX>h2o</NODENAME_REGEX>
    <OS>CNL</OS>
    <COMPILERS>intel,pgi,cray,gnu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>banu</PROJECT>
    <CIME_OUTPUT_ROOT>/scratch/sciteam/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{CESMDATAROOT}/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{CESMDATAROOT}/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESMDATAROOT}/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{CESMDATAROOT}/tools/cprnc</CCSM_CPRNC>
    <GMAKE_J> 8</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>16</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>aprun</executable>
      <arguments>
	<arg name="num_tasks"> -n {{ total_tasks }}</arg>
	<!-- <arg name="tasks_per_numa"> -S {{ tasks_per_numa }}</arg> -->
	<arg name="tasks_per_node"> -N $MAX_MPITASKS_PER_NODE</arg>
	<arg name="thread_count"> -d $ENV{OMP_NUM_THREADS}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/modules/default/init/perl.pm</init_path>
      <init_path lang="python">/opt/modules/default/init/python.py</init_path>
      <init_path lang="sh">/opt/modules/default/init/sh</init_path>
      <init_path lang="csh">/opt/modules/default/init/csh</init_path>
      <cmd_path lang="perl">/opt/modules/3.2.10.3/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/opt/modules/3.2.10.3/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="rm">PrgEnv-pgi</command>
	<command name="rm">PrgEnv-intel</command>
	<command name="rm">PrgEnv-cray</command>
	<command name="rm">PrgEnv-gnu</command>
	<command name="rm">pgi</command>
	<command name="rm">cray</command>
	<command name="rm">intel</command>
	<command name="rm">cray-netcdf</command>
	<command name="rm">gcc</command>
      </modules>
      <modules compiler="intel">
	<command name="load">PrgEnv-intel</command>
	<command name="rm">intel</command>
	<command name="load">intel/18.0.3.222</command>
	<!-- the PrgEnv-intel loads a gcc compiler that causes several
	problems -->
	<command name="rm">gcc</command>
      </modules>
      <modules compiler="pgi">
	<command name="load">PrgEnv-pgi</command>
	<command name="switch">pgi pgi/18.7.0</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">PrgEnv-gnu</command>
	<command name="switch">gcc gcc/6.3.0</command>
      </modules>
      <modules compiler="cray">
	<command name="load">PrgEnv-cray</command>
	<command name="switch">cce cce/8.5.8</command>
      </modules>
      <modules>
	<command name="load">papi/5.5.1.1</command>
	<command name="switch">cray-mpich cray-mpich/7.7.1</command>
	<command name="switch">cray-libsci cray-libsci/18.04.1</command>
	<command name="load">torque/6.0.4</command>
      </modules>
      <modules mpilib="!mpi-serial">
	<command name="load">cray-hdf5-parallel/1.10.2.0</command>
	<command name="load">cray-netcdf-hdf5parallel/4.6.1.0</command>
	<command name="load">cray-parallel-netcdf/1.8.1.3</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">cray-netcdf/4.6.1.0</command>
      </modules>
      <modules>
	<command name="load">cmake/3.1.3</command>
	<command name="rm">darshan</command>
	<command name="use">/sw/modulefiles/CESM</command>
	<command name="load">CESM-ENV</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="PATH">$ENV{HOME}/bin:$ENV{PATH}</env>
    </environment_variables>
  </machine>

  <machine MACH="centos7-linux">
    <DESC>
      Example port to centos7 linux system with gcc, netcdf, pnetcdf and mpich
      using modules from http://www.admin-magazine.com/HPC/Articles/Environment-Modules
    </DESC>
    <NODENAME_REGEX>regex.expression.matching.your.machine</NODENAME_REGEX>
    <OS>LINUX</OS>
    <PROXY> https://howto.get.out </PROXY>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>none</PROJECT>
    <SAVE_TIMING_DIR> </SAVE_TIMING_DIR>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/cesm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/cesm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/cesm/inputdata/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{HOME}/cesm/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{HOME}/cesm/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{HOME}/cesm/tools/cime/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>me@my.address</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>8</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>8</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec</executable>
      <arguments>
	<arg name="ntasks"> -np {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
      </modules>
      <modules compiler="gnu">
	<command name="load">compiler/gnu/8.2.0</command>
	<command name="load">mpi/3.3/gcc-8.2.0</command>
	<command name="load">tool/netcdf/4.6.1/gcc-8.1.0</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="cheyenne">
    <DESC>NCAR SGI platform, os is Linux, 36 pes/node, batch system is PBS</DESC>
    <NODENAME_REGEX>.*.?cheyenne\d?.ucar.edu</NODENAME_REGEX>
    <!-- MPT sometimes timesout at model start time, the next two lines cause
    case_run.py to detect the timeout and retry FORCE_SPARE_NODES times -->
    <MPIRUN_RETRY_REGEX>MPT: xmpi_net_accept_timeo/accept() timeout</MPIRUN_RETRY_REGEX>
    <MPIRUN_RETRY_COUNT>10</MPIRUN_RETRY_COUNT>
    <OS>LINUX</OS>
    <COMPILERS>intel,gnu,pgi</COMPILERS>
    <MPILIBS compiler="intel" >mpt,openmpi</MPILIBS>
    <MPILIBS compiler="pgi" >openmpi,mpt</MPILIBS>
    <MPILIBS compiler="gnu" >openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/glade/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{CESMDATAROOT}/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/glade/p/cgd/tss/CTSM_datm_forcing_data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESMDATAROOT}/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{CESMDATAROOT}/tools/cime/tools/cprnc/cprnc.cheyenne</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <!-- have not seen any performance benefit in smt -->
    <MAX_TASKS_PER_NODE>36</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>36</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec_mpt</executable>
      <arguments>
	<!-- for mpt/2.19 the -p needs to preceed -np -->
	<arg name="labelstdout">-p "%g:"</arg>
	<arg name="num_tasks"> -np {{ total_tasks }}</arg>
	<!-- the omplace argument needs to be last -->
	<arg name="zthreadplacement"> omplace -tm open64 </arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpt" queue="share">
      <executable>mpirun `hostname`</executable>
      <arguments>
	<arg name="anum_tasks"> -np {{ total_tasks }}</arg>
	<!-- the omplace argument needs to be last -->
	<arg name="zthreadplacement"> omplace -tm open64 </arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
	<arg name="anum_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="default" unit_testing="true">
      <!-- The only place we can build and run the unit tests is on cheyenne's
	   shared nodes. However, running mpi jobs on the shared nodes currently
	   requires some workarounds; these workarounds are implemented here -->
      <executable>/opt/sgi/mpt/mpt-2.15/bin/mpirun $ENV{UNIT_TEST_HOST} -np 1 </executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">ncarenv/1.2</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/17.0.1</command>
	<command name="load">esmf_libs</command>
	<command name="load">mkl</command>
      </modules>
      <modules compiler="pgi">
	<command name="load">pgi/19.3</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial" DEBUG="TRUE">
	<command name="load">esmf-7.1.0r-defio-mpi-g</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial" DEBUG="FALSE">
	<command name="load">esmf-7.1.0r-defio-mpi-O</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial" DEBUG="TRUE">
	<command name="load">esmf-7.1.0r-ncdfio-uni-g</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial" DEBUG="FALSE">
	<command name="load">esmf-7.1.0r-ncdfio-uni-O</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">gnu/7.3.0</command>
	<command name="load">openblas/0.2.20</command>
      </modules>
      <modules mpilib="mpt" compiler="gnu">
	<command name="load">mpt/2.16</command>
	<command name="load">netcdf-mpi/4.6.1</command>
      </modules>
      <modules mpilib="mpt" compiler="intel">
	<command name="load">mpt/2.19</command>
	<command name="load">netcdf-mpi/4.6.1</command>
	<command name="load">pnetcdf/1.11.0</command>
      </modules>
      <modules mpilib="mpt" compiler="pgi">
	<command name="load">mpt/2.19</command>
	<command name="load">netcdf-mpi/4.6.3</command>
	<command name="load">pnetcdf/1.11.1</command>
      </modules>
      <modules mpilib="openmpi" compiler="pgi">
	<command name="load">openmpi/3.1.4</command>
	<command name="load">netcdf/4.6.3</command>
      </modules>
      <modules mpilib="openmpi" compiler="gnu">
	<command name="load">openmpi/3.0.1</command>
	<command name="load">netcdf/4.6.1</command>
      </modules>
      <modules>
	<command name="load">ncarcompilers/0.5.0</command>
      </modules>
      <modules compiler="gnu" mpilib="mpi-serial">
	<command name="load">netcdf/4.6.1</command>
      </modules>
      <modules compiler="pgi" mpilib="mpi-serial">
	<command name="load">netcdf/4.6.3</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial">
	<command name="load">netcdf/4.5.0</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="TMPDIR">/glade/scratch/$USER</env>
      <env name="MPI_TYPE_DEPTH">16</env>
      <env name="MPI_IB_CONGESTED">1</env>
      <env name="MPI_USE_ARRAY"/>
    </environment_variables>
    <environment_variables unit_testing="true">
      <env name="MPI_USE_ARRAY">false</env>
    </environment_variables>
    <environment_variables queue="share">
      <env name="TMPDIR">/glade/scratch/$USER</env>
      <env name="MPI_USE_ARRAY">false</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="comet">
    <DESC>
      Comet is a dedicated eXtreme Science and Engineering Discovery
      Environment (XSEDE) cluster designed by Dell and SDSC delivering 2.76
      peak petaflops. It features Intel next-gen processors with AVX2,
      Mellanox FDR InfiniBand interconnects, and Aeon storage.
      https://www.sdsc.edu/support/user_guides/comet.html
    </DESC>
    <NODENAME_REGEX>comet.*.sdsc.edu</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi,impi,mvapich</MPILIBS>
    <CIME_OUTPUT_ROOT>/cw3e/mead/projects/csg100/cesm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/cw3e/mead/projects/csg100/cesm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/cw3e/mead/projects/csg100/cesm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/cw3e/mead/projects/csg100/cesm/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/cw3e/mead/projects/csg100/cesm/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/cw3e/mead/projects/csg100/cesm/tools/bin/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>jedwards -at- ucar.edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>24</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="openmpi">
      <executable>ibrun -v</executable>
      <arguments>
	<arg name="num_tasks">-n {{ total_tasks }}</arg>
	<arg name="bind"> -bp scatter -bl core</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mvapich">
      <executable>ibrun -v</executable>
      <arguments>
	<arg name="num_tasks">-n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="impi">
      <executable>srun </executable>
      <arguments>
	<arg name="num_tasks">-n {{ total_tasks }}</arg>
	<arg name="cpu_bind">--cpu_bind=sockets --cpu_bind=verbose</arg>
	<arg name="qos">--qos=cw3e</arg>
      </arguments>
    </mpirun>

    <module_system type="module">
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <cmd_path lang="python">/bin/modulecmd python </cmd_path>
      <cmd_path lang="perl">/bin/modulecmd perl </cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
      </modules>
      <modules>
	<command name="use">/share/apps/compute/modulefiles</command>
	<command name="load">applications/cmake/3.17.0</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/2019.5.281</command>
	<command name="load">mkl</command>
      </modules>
      <modules mpilib="openmpi">
	<command name="unload">mvapich2_ib/2.3.2</command>
	<command name="load">openmpi_ib/3.1.4</command>
	<command name="load">netcdf-serial/4.6.1</command>
      </modules>
      <modules mpilib="mvapich">
	<command name="load">mvapich2_ib/2.3.2</command>
	<command name="load">netcdf/4.6.1</command>
      </modules>
      <modules mpilib="impi">
	<command name="unload">mvapich2_ib/2.3.2</command>
	<command name="load">intelmpi/2019.5.281</command>
	<command name="load">netcdf/4.6.1</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">netcdf-serial/4.6.1</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <environment_variables mpilib="impi">
<!--      <env name="TMI_CONFIG">$ENV{MPIHOME}/etc/tmi.conf</env> -->
      <env name="I_MPI_PMI_LIBRARY">/usr/lib64/libpmi.so</env>
      <env name="PNETCDF_PATH">/home/edwardsj/work/cesm/tools/pnetcdf/intel/impi.2019</env>
    </environment_variables>
    <environment_variables mpilib="mvapich">
      <env name="PNETCDF_PATH">/home/edwardsj/work/cesm/tools/pnetcdf/intel/mvapich</env>
    </environment_variables>
    <environment_variables mpilib="openmpi">
      <env name="PNETCDF_PATH">/home/edwardsj/work/cesm/tools/pnetcdf/intel/openmpi</env>
    </environment_variables>
  </machine>

  <machine MACH="constance">
    <DESC>PNL Haswell cluster, OS is Linux, batch system is SLURM</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel,pgi</COMPILERS>
    <MPILIBS>mvapich2,openmpi,intelmpi,mvapich</MPILIBS>
    <CIME_OUTPUT_ROOT>/pic/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/pic/scratch/tcraig/IRESM/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/pic/scratch/tcraig/IRESM/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/pic/scratch/$USER/cases/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/pic/scratch/tcraig/IRESM/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/people/tcraig/bin/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>tcraig -at- ucar.edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>24</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="mvapich2">
      <executable>srun</executable>
      <arguments>
	<arg name="mpi">--mpi=none</arg>
	<arg name="num_tasks">--ntasks={{ total_tasks }}</arg>
	<arg name="cpu_bind">--cpu_bind=sockets --cpu_bind=verbose</arg>
	<arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mvapich">
      <executable>srun</executable>
      <arguments>
	<arg name="num_tasks">--ntasks={{ total_tasks }}</arg>
	<arg name="cpu_bind">--cpu_bind=sockets --cpu_bind=verbose</arg>
	<arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="intelmpi">
      <executable>mpirun</executable>
      <arguments>
	<arg name="num_tasks">-n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
	<arg name="num_tasks">-n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/share/apps/modules/Modules/3.2.10/init/perl.pm</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <cmd_path lang="perl">/share/apps/modules/Modules/3.2.10/bin/modulecmd perl </cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
      </modules>
      <modules>
	<command name="load">perl/5.20.0</command>
	<command name="load">cmake/2.8.12</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/15.0.1</command>
	<command name="load">netcdf/4.3.2</command>
	<command name="load">mkl/15.0.1</command>
      </modules>
      <modules compiler="pgi">
	<command name="load">pgi/14.10</command>
	<command name="load">netcdf/4.3.2</command>
      </modules>
      <modules mpilib="mvapich">
	<command name="load">mvapich2/2.1</command>
      </modules>
      <modules mpilib="mvapich2">
	<command name="load">mvapich2/2.1</command>
      </modules>
      <modules mpilib="intelmpi">
	<command name="load">intelmpi/5.0.1.035</command>
      </modules>
      <modules mpilib="openmpi">
	<command name="load">openmpi/1.8.3</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="MKL_PATH">$MLIB_LIB</env>
      <env name="NETCDF_HOME">/share/apps/netcdf/4.3.2/intel/15.0.1</env>
    </environment_variables>
    <environment_variables compiler="pgi">
      <env name="NETCDF_HOME">/share/apps/netcdf/4.3.2/pgi/14.10</env>
    </environment_variables>
  </machine>

  <machine MACH="perlmutter">
    <DESC>Perlmutter CPU-only nodes at NERSC.  Phase2 only: Each node has 2 AMD EPYC 7713 64-Core (Milan) 512GB batch system is Slurm</DESC>
<!--    <NODENAME_REGEX>*.chn</NODENAME_REGEX> -->
    <OS>Linux</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{PSCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/global/cfs/cdirs/ccsm1/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/global/cfs/cdirs/ccsm1/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/global/cfs/cdirs/ccsm1/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/global/cfs/cdirs/ccsm1/tools/cprnc.perlmutter/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>256</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
	<arg name="label"> --label</arg>
	<arg name="num_tasks" > -n {{ total_tasks }} -N {{ num_nodes }}</arg>
	<arg name="thread_count">-c $SHELL{echo 256/`./xmlquery --value MAX_MPITASKS_PER_NODE`|bc}</arg>
        <arg name="binding"> $SHELL{if [ 128 -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;} </arg>
        <arg name="placement"> -m plane=$SHELL{echo `./xmlquery --value MAX_MPITASKS_PER_NODE`}</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/usr/share/lmod/8.3.1/init/perl</init_path>
      <init_path lang="python">/usr/share/lmod/8.3.1/init/python</init_path>
      <init_path lang="sh">/usr/share/lmod/8.3.1/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/8.3.1/init/csh</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules>
        <command name="unload">cray-hdf5-parallel</command>
        <command name="unload">cray-netcdf-hdf5parallel</command>
        <command name="unload">cray-parallel-netcdf</command>
        <command name="unload">cray-netcdf</command>
        <command name="unload">cray-hdf5</command>
        <command name="unload">PrgEnv-gnu</command>
        <command name="unload">PrgEnv-intel</command>
        <command name="unload">PrgEnv-nvidia</command>
        <command name="unload">PrgEnv-cray</command>
        <command name="unload">PrgEnv-aocc</command>
        <command name="unload">intel</command>
        <command name="unload">intel-oneapi</command>
        <command name="unload">nvidia</command>
        <command name="unload">aocc</command>
        <command name="unload">cudatoolkit</command>
        <command name="unload">climate-utils</command>
        <command name="unload">craype-accel-nvidia80</command>
        <command name="unload">craype-accel-host</command>
        <command name="unload">perftools-base</command>
        <command name="unload">perftools</command>
        <command name="unload">darshan</command>
      </modules>

      <modules compiler="gnu">
        <command name="load">PrgEnv-gnu/8.3.3</command>
        <command name="load">gcc/11.2.0</command>
        <command name="load">cray-libsci/23.02.1.1</command>
      </modules>

      <modules compiler="intel">
        <command name="load">PrgEnv-intel/8.3.3</command>
        <command name="load">intel/2023.1.0</command>
      </modules>

      <modules compiler="nvidia">
        <command name="load">PrgEnv-nvidia</command>
        <command name="load">nvidia/22.7</command>
        <command name="load">cray-libsci/23.02.1.1</command>
      </modules>

      <modules compiler="amdclang">
        <command name="load">PrgEnv-aocc</command>
        <command name="load">aocc/4.0.0</command>
        <command name="load">cray-libsci/23.02.1.1</command>
      </modules>

      <modules>
        <command name="load">craype-accel-host</command>
        <command name="load">craype/2.7.20</command>
        <command name="load">cray-mpich/8.1.25</command>
        <command name="load">cray-hdf5-parallel/1.12.2.3</command>
        <command name="load">cray-netcdf-hdf5parallel/4.9.0.3</command>
        <command name="load">cray-parallel-netcdf/1.12.3.3</command>
        <command name="load">cmake/3.24.3</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
      <env name="PERL5LIB">/global/cfs/cdirs/e3sm/perl/lib/perl5-only-switch</env>
      <env name="FI_CXI_RX_MATCH_MODE">software</env>
      <env name="MPICH_COLL_SYNC">MPI_Bcast</env>
      <env name="NETCDF_PATH">$ENV{CRAY_NETCDF_HDF5PARALLEL_PREFIX}</env>
      <env name="PNETCDF_PATH">$ENV{CRAY_PARALLEL_NETCDF_PREFIX}</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="daint">
    <DESC>CSCS Cray XC50, os is SUSE SLES, 12 pes/node, batch system is SLURM</DESC>
    <OS>CNL</OS>
    <COMPILERS>pgi,cray,gnu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>/scratch/snx3000/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/project/s824/cesm_inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/project/s824/cesm_inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/project/s824/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/project/s824/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/project/s824/cesm_tools/ccsm_cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>12</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>edouard.davin -at- env.ethz.ch</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>12</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>12</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
	<arg name="num_tasks"> -n {{ total_tasks }}</arg>
	<arg name="thread_count"> -d $ENV{OMP_NUM_THREADS}</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
  </machine>

  <machine MACH="derecho">
    <DESC>NCAR AMD EPYC </DESC>
    <NODENAME_REGEX>de.*.hpc.ucar.edu</NODENAME_REGEX>
    <OS>CNL</OS>
    <COMPILERS>intel,gnu,nvhpc</COMPILERS>
<!--    <COMPILERS>intel,gnu,cray,nvhpc,intel-oneapi,intel-classic</COMPILERS> -->
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{CESMDATAROOT}/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{CESMDATAROOT}/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESMDATAROOT}/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{CESMDATAROOT}/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>16</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec</executable>
      <arguments>
	<arg name="label"> --label</arg>
	<arg name="buffer"> --line-buffer</arg>
	<arg name="num_tasks" > -n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">$LMOD_ROOT/lmod/init/perl</init_path>
      <init_path lang="python">$LMOD_ROOT/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">$LMOD_ROOT/lmod/init/sh</init_path>
      <init_path lang="csh">$LMOD_ROOT/lmod/init/csh</init_path>
      <cmd_path lang="perl">$LMOD_ROOT/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">$LMOD_ROOT/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="load">cesmdev/1.0</command>
	<command name="load">ncarenv/24.12</command>
	<command name="purge"/>
	<command name="load">craype</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/2024.2.1</command>
	<command name="load">mkl</command>
<!--	<command name="load">spherepack/3.2</command> -->
      </modules>
      <modules compiler="cray">
	<command name="load">cce/17.0.1</command>
	<command name="load">cray-libsci/24.03.0</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">gcc/12.4.0</command>
	<command name="load">cray-libsci/24.03.0</command>
      </modules>
      <modules compiler="nvhpc">
	<command name="load">nvhpc/24.11</command>
      </modules>
      <modules>
	<command name="load">ncarcompilers/1.0.0</command>
	<command name="load">cmake</command>
      </modules>
      <modules mpilib="mpich">
	<command name="load">cray-mpich/8.1.29</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">mpi-serial/2.5.0</command>
      </modules>

      <modules mpilib="mpi-serial">
	<command name="load">netcdf/4.9.2</command>
      </modules>

      <modules mpilib="!mpi-serial">
	<command name="load">netcdf-mpi/4.9.2</command>
	<command name="load">parallel-netcdf/1.14.0</command>
      </modules>

      <modules>
        <command name="load">esmf/8.7.0</command>
      </modules>

    </module_system>

    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="FI_CXI_RX_MATCH_MODE">hybrid</env>
      <env name="FI_MR_CACHE_MONITOR">memhooks</env>
<!--      <env name="SPHEREPACK_LIBDIR">$ENV{NCAR_ROOT_SPHEREPACK}/lib</env> -->
    </environment_variables>

  </machine>

  <machine MACH="eastwind">
    <DESC>PNL IBM Xeon cluster, os is Linux (pgi), batch system is SLURM</DESC>
    <OS>LINUX</OS>
    <COMPILERS>pgi,intel</COMPILERS>
    <MPILIBS>mvapich2,mvapich</MPILIBS>
    <CIME_OUTPUT_ROOT>/lustre/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/lustre/tcraig/IRESM/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lustre/tcraig/IRESM/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/lustre/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lustre/tcraig/IRESM/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/lustre/tcraig/IRESM/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>tcraig -at- ucar.edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>12</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>12</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mvapich">
      <executable>srun</executable>
      <arguments>
	<arg name="num_tasks"> --ntasks={{ total_tasks }}</arg>
	<arg name="cpubind"> --cpu_bind=sockets</arg>
	<arg name="cpubind"> --cpu_bind=verbose</arg>
	<arg name="killonbadexit"> --kill-on-bad-exit</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mvapich2">
      <executable>srun</executable>
      <arguments>
	<arg name="mpinone"> --mpi=none</arg>
	<arg name="num_tasks"> --ntasks={{ total_tasks }}</arg>
	<arg name="cpubind"> --cpu_bind=sockets</arg>
	<arg name="cpubind"> --cpu_bind=verbose</arg>
	<arg name="killonbadexit"> --kill-on-bad-exit</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/etc/profile.d/modules.perl</init_path>
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <cmd_path lang="perl">/share/apps/modules/Modules/3.2.7/bin/modulecmd perl</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">perl/5.20.7</command>
	<command name="load">cmake/3.0.0</command>
	<command name="load">pgi/15.5</command>
	<command name="load">mpi/mvapich2/1.5.1p1/pgi11.3</command>
	<command name="load">netcdf/4.1.2/pgi</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
  </machine>

  <machine MACH="euler2">
    <DESC>Euler II Linux Cluster ETH, 24 pes/node, InfiniBand, XeonE5_2680v3, batch system LSF</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel,pgi</COMPILERS>
    <MPILIBS>openmpi,mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>/cluster/work/climate/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/cluster/work/climate/cesm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/cluster/work/climate/cesm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/cluster/work/climate/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/cluster/work/climate/cesm/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/cluster/work/climate/cesm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>1</GMAKE_J>
    <BATCH_SYSTEM>lsf</BATCH_SYSTEM>
    <SUPPORTED_BY>urs.beyerle -at- env.ethz.ch</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>24</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mpich">
      <executable>mpirun</executable>
      <arguments>
	<arg name="machine_file">-hostfile $ENV{PBS_JOBID}</arg>
	<arg name="tasks_per_node"> -ppn $MAX_MPITASKS_PER_NODE</arg>
	<arg name="num_tasks"> -n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/cluster/apps/modules/init/python.py</init_path>
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <cmd_path lang="python">/cluster/apps/modules/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
      </modules>
      <modules>
	<command name="load">new</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/2018.1</command>
      </modules>
      <modules>
	<command name="load">netcdf/4.3.1</command>
      </modules>
      <modules compiler="pgi">
	<command name="load">pgi/14.1</command>
      </modules>
      <modules mpilib="mpich">
	<command name="load">mvapich2/1.8.1</command>
      </modules>
      <modules mpilib="openmpi">
	<command name="load">open_mpi/1.6.5</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
  </machine>

  <machine MACH="euler3">
    <DESC>Euler III Linux Cluster ETH, 4 pes/node, Ethernet, XeonE3_1585Lv5, batch system LSF</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel,pgi</COMPILERS>
    <MPILIBS>openmpi,mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>/cluster/work/climate/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/cluster/work/climate/cesm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/cluster/work/climate/cesm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/cluster/work/climate/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/cluster/work/climate/cesm/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/cluster/work/climate/cesm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>1</GMAKE_J>
    <BATCH_SYSTEM>lsf</BATCH_SYSTEM>
    <SUPPORTED_BY>urs.beyerle -at- env.ethz.ch</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>4</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>4</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mpich">
      <executable>mpirun</executable>
      <arguments>
	<arg name="machine_file">-hostfile $ENV{PBS_JOBID}</arg>
	<arg name="tasks_per_node"> -ppn $MAX_MPITASKS_PER_NODE</arg>
	<arg name="num_tasks"> -n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/cluster/apps/modules/init/python.py</init_path>
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <cmd_path lang="python">/cluster/apps/modules/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
      </modules>
      <modules>
	<command name="load">new</command>
      </modules>
      <modules>
	<command name="load">interconnect/ethernet</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/2018.1</command>
      </modules>
      <modules>
	<command name="load">netcdf/4.3.1</command>
      </modules>
      <modules compiler="pgi">
	<command name="load">pgi/14.1</command>
      </modules>
      <modules mpilib="mpich">
	<command name="load">mvapich2/1.8.1</command>
      </modules>
      <modules mpilib="openmpi">
	<command name="load">open_mpi/1.6.5</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
  </machine>

  <machine MACH="euler4">
    <DESC>Euler IV Linux Cluster ETH, 36 pes/node, InfiniBand, XeonGold_6150, batch system LSF</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel,pgi</COMPILERS>
    <MPILIBS>openmpi,mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>/cluster/work/climate/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/cluster/work/climate/cesm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/cluster/work/climate/cesm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/cluster/work/climate/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/cluster/work/climate/cesm/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/cluster/work/climate/cesm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>1</GMAKE_J>
    <BATCH_SYSTEM>lsf</BATCH_SYSTEM>
    <SUPPORTED_BY>urs.beyerle -at- env.ethz.ch</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>36</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>36</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mpich">
      <executable>mpirun</executable>
      <arguments>
	<arg name="machine_file">-hostfile $ENV{PBS_JOBID}</arg>
	<arg name="tasks_per_node"> -ppn $MAX_MPITASKS_PER_NODE</arg>
	<arg name="num_tasks"> -n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/cluster/apps/modules/init/python.py</init_path>
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <cmd_path lang="python">/cluster/apps/modules/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
      </modules>
      <modules>
	<command name="load">new</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/2018.1</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
  </machine>

  <machine MACH="gaea">
    <DESC>NOAA XE6, os is CNL, 24 pes/node, batch system is PBS</DESC>
    <OS>CNL</OS>
    <COMPILERS>pgi</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>/lustre/fs/scratch/Julio.T.Bacmeister</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/lustre/fs/scratch/Julio.T.Bacmeister/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lustre/fs/scratch/Julio.T.Bacmeister/inputdata</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/lustre/fs/scratch/Julio.T.Bacmeister/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>UNSET</BASELINE_ROOT>
    <CCSM_CPRNC>UNSET</CCSM_CPRNC>
    <GMAKE_J> 8</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>julio -at- ucar.edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>24</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>aprun</executable>
      <arguments>
	<arg name="hyperthreading" default="2"> -j {{ hyperthreading }}</arg>
	<arg name="num_tasks" > -n {{ total_tasks }}</arg>
	<arg name="tasks_per_numa" > -S {{ tasks_per_numa }}</arg>
	<arg name="tasks_per_node" > -N $MAX_MPITASKS_PER_NODE</arg>
	<arg name="thread_count" > -d $ENV{OMP_NUM_THREADS}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/modules/default/init/perl.pm</init_path>
      <init_path lang="csh">/opt/modules/default/init/csh</init_path>
      <init_path lang="sh">/opt/modules/default/init/sh</init_path>
      <cmd_path lang="perl">/opt/modules/default/bin/modulecmd perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="rm">PrgEnv-pgi</command>
	<command name="rm">PrgEnv-cray</command>
	<command name="rm">PrgEnv-gnu</command>
	<command name="rm">pgi</command>
	<command name="rm">cray</command>
      </modules>
      <modules compiler="pgi">
	<command name="load">PrgEnv-pgi</command>
	<command name="switch">pgi pgi/12.5.0</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">PrgEnv-gnu</command>
	<command name="load">torque</command>
      </modules>
      <modules compiler="cray">
	<command name="load">PrgEnv-cray/4.0.36</command>
	<command name="load">cce/8.0.2</command>
      </modules>
      <modules>
	<command name="load">torque/4.1.3</command>
	<command name="load">netcdf-hdf5parallel/4.2.0</command>
	<command name="load">parallel-netcdf/1.2.0</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="MPICH_ENV_DISPLAY">1</env>
    </environment_variables>
  </machine>

  <machine MACH="grace">
    <DESC>Intel Xeon 6248R 3.0 GHz ("Cascade Lake"),48 cores on two sockets (24 cores/socket) , batch system is SLURM</DESC>
    <NODENAME_REGEX>.*grace</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/scratch/group/ihesp/software/cesm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/scratch/group/ihesp/software/cesm/inputdata/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/scratch/group/ihesp/software/cesm/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/scratch/group/ihesp/software/cesm/cime/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>agopal</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>96</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>48</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="impi">
      <executable>srun</executable>
      <arguments>
	<arg name="num_tasks"> -n $TOTALPES</arg>
	<arg name="hint"> --hint compute_bound </arg>
	<arg name="bindinfo"> --cpu-bind=verbose </arg>
	<!--	<arg name="bindinfo"> -print-rank-map </arg> -->
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/sw/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/sw/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/sw/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/sw/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/sw/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/sw/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"></command>
	<command name="load">intel/2019b</command>
	<command name="load">CMake/3.15.3</command>
	<command name="load">cURL/7.66.0</command>
	<command name="load">Python/2.7.16</command>
	<command name="load">XML-LibXML/2.0201</command>
      </modules>
      <modules mpilib="impi">
	<command name="load">impi/2018.5.288</command>
	<command name="load">HDF5/1.10.5</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="I_MPI_PMI_LIBRARY">/usr/lib64/libpmi.so</env>
      <env name="I_MPI_DEBUG">+4</env>
    </environment_variables>
  </machine>

  <machine MACH="gust">
    <DESC>NCAR AMD EPYC test system 16 CPU nodes 2 GPU nodes</DESC>
    <NODENAME_REGEX>gu.*.hpc.ucar.edu</NODENAME_REGEX>
    <OS>CNL</OS>
    <COMPILERS>intel</COMPILERS>
<!--    <COMPILERS>intel,gnu,cray,nvhpc,intel-oneapi,intel-classic</COMPILERS> -->
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/glade/p/cesmdata/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/glade/p/cesmdata/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/glade/p/cesmdata/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/glade/p/cesmdata/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>16</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec</executable>
      <arguments>
	<arg name="label"> --label</arg>
	<arg name="num_tasks" > -n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/glade/u/apps/$NCAR_HOST/$NCAR_ENV_VERSION/spack/opt/spack/lmod/8.7.14/gcc/7.5.0/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/glade/u/apps/$NCAR_HOST/$NCAR_ENV_VERSION/spack/opt/spack/lmod/8.7.14/gcc/7.5.0/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/glade/u/apps/$NCAR_HOST/$NCAR_ENV_VERSION/spack/opt/spack/lmod/8.7.14/gcc/7.5.0/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/glade/u/apps/$NCAR_HOST/$NCAR_ENV_VERSION/spack/opt/spack/lmod/8.7.14/gcc/7.5.0/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/glade/u/apps/$NCAR_HOST/$NCAR_ENV_VERSION/spack/opt/spack/lmod/8.7.14/gcc/7.5.0/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/glade/u/apps/$NCAR_HOST/$NCAR_ENV_VERSION/spack/opt/spack/lmod/8.7.14/gcc/7.5.0/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="load">cesmdev/1.0</command>
	<command name="load">ncarenv/23.04</command>
	<command name="purge"/>
	<command name="load">craype</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/2023.0.0</command>
	<command name="load">mkl</command>
      </modules>
      <modules compiler="intel-oneapi">
	<command name="load">intel-oneapi/2023.0.0</command>
	<command name="load">mkl</command>
      </modules>
      <modules compiler="intel-classic">
	<command name="load">intel-classic/2023.0.0</command>
	<command name="load">mkl</command>
      </modules>
      <modules compiler="cray">
	<command name="load">cce/15.0.1</command>
	<command name="load">cray-libsci/23.02.1.1</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">gcc/12.2.0</command>
	<command name="load">cray-libsci/23.02.1.1</command>
      </modules>
      <modules compiler="nvhpc">
	<command name="load">nvhpc/23.1</command>
      </modules>
      <modules>
	<command name="load">ncarcompilers/0.8.0</command>
	<command name="load">cmake</command>
      </modules>
      <modules mpilib="mpich">
	<command name="load">cray-mpich/8.1.25</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">mpi-serial/2.3.0</command>
      </modules>

      <modules mpilib="mpi-serial">
	<command name="load">netcdf/4.9.1</command>
      </modules>

      <modules mpilib="!mpi-serial">
	<command name="load">netcdf-mpi/4.9.1</command>
	<command name="load">parallel-netcdf/1.12.3</command>
	<command name="load">esmf/8.4.1</command>
      </modules>

    </module_system>

    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="FI_CXI_RX_MATCH_MODE">hybrid</env>
    </environment_variables>
    <environment_variables mpilib="mpich">
      <env name="MPICH_MPIIO_HINTS">*:romio_cb_read=enable:romio_cb_write=enable:striping_factor=2</env>
    </environment_variables>
  </machine>

  <machine MACH="hobart">
    <DESC>NCAR CGD Linux Cluster 48 pes/node, batch system is PBS</DESC>
    <NODENAME_REGEX>^h.*\.cgd\.ucar\.edu</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,pgi,nag,gnu</COMPILERS>
    <MPILIBS>mvapich2,openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/scratch/cluster/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/fs/cgd/csm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/project/tss</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/scratch/cluster/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/fs/cgd/csm/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/fs/cgd/csm/tools/cime/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE>gmake --output-sync</GMAKE>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY> cseg </SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>48</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>48</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mvapich2">
      <executable>mpiexec</executable>
      <arguments>
	<arg name="machine_file">--machinefile $ENV{PBS_NODEFILE}</arg>
	<arg name="num_tasks"> -n {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpiexec</executable>
      <arguments>
	<arg name="num_tasks"> -n {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="purge"></command>
      </modules>
      <modules compiler="intel">
	<command name="load">compiler/intel/18.0.3</command>
	<command name="load">tool/netcdf/4.6.1/intel</command>
      </modules>
      <modules compiler="intel" mpilib="mvapich2">
	<command name="load">mpi/intel/mvapich2-2.3rc2-intel-18.0.3</command>
      </modules>
      <modules compiler="pgi">
	<command name="load">compiler/pgi/18.1</command>
	<command name="load">tool/netcdf/4.6.1/pgi</command>
      </modules>
      <modules compiler="nag">
	<command name="load">compiler/nag/6.2</command>
	<command name="load">tool/netcdf/4.6.1/nag</command>
      </modules>
      <modules compiler="nag" mpilib="mvapich2">
	<command name="load">mpi/nag/mvapich2-2.3rc2</command>
      </modules>
      <modules compiler="nag" mpilib="openmpi">
	<command name="load">mpi/nag/openmpi-3.1.0</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">compiler/gnu/8.1.0</command>
	<command name="load">tool/netcdf/4.6.1/gcc</command>
      </modules>
      <modules compiler="gnu" mpilib="openmpi">
	<command name="load">mpi/gcc/openmpi-3.1.0a</command>
      </modules>
      <modules compiler="gnu" mpilib="mvapich2">
	<command name="load">mpi/gcc/mvapich2-2.3rc2-qlc</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
      <!-- The following is needed to access qsub from the compute nodes -->
      <env name="PATH">$ENV{PATH}:/cluster/torque/bin</env>
      <env name="ESMFMKFILE">/home/dunlap/ESMF-INSTALL/8.0.0bs16/lib/libg/Linux.intel.64.mvapich2.default/esmf.mk</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="homebrew">
    <DESC>

      Customize these fields as appropriate for your system,
      particularly changing MAX_TASKS_PER_NODE and MAX_MPITASKS_PER_NODE to the
      number of cores on your machine.  You may also want to change
      instances of '$ENV{HOME}/projects' to your desired directory
      organization.  You can use this in either of two ways: (1)
      Without making any changes, by adding `--machine homebrew` to
      create_newcase or create_test (2) Copying this into a
      config_machines.xml file in your personal .cime directory and
      then changing the machine name (MACH="homebrew") to
      your machine name and the NODENAME_REGEX to something matching
      your machine's hostname.  With (2), you should not need the
      `--machine` argument, because the machine should be determined
      automatically.  However, with (2), you will also need to copy the
      homebrew-specific settings in config_compilers.xml into a
      config_compilers.xml file in your personal .cime directory, again
      changing the machine name (MACH="homebrew") to your machine name.

    </DESC>
    <NODENAME_REGEX> something.matching.your.machine.hostname </NODENAME_REGEX>
    <OS>Darwin</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/projects/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/projects/cesm-inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/projects/ptclm-data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{HOME}/projects/scratch/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{HOME}/projects/baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$CIMEROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>__YOUR_NAME_HERE__</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>8</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>4</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
	<arg name="anum_tasks"> -np {{ total_tasks }}</arg>
	<arg name="labelstdout">-prepend-rank</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
    <environment_variables>
      <env name="NETCDF_PATH">/usr/local</env>
    </environment_variables>
  </machine>

  <machine MACH="izumi">
    <DESC>NCAR CGD Linux Cluster 48 pes/node, batch system is PBS</DESC>
    <NODENAME_REGEX>^i.*\.ucar\.edu</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,pgi,nag,gnu</COMPILERS>
    <MPILIBS>mvapich2,openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/scratch/cluster/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/fs/cgd/csm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/project/tss</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/scratch/cluster/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/fs/cgd/csm/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/fs/cgd/csm/tools/cime/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE>gmake --output-sync</GMAKE>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY> cseg </SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>48</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>48</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mvapich2">
      <executable>mpiexec</executable>
      <arguments>
	<arg name="machine_file">--machinefile $ENV{PBS_NODEFILE}</arg>
	<arg name="num_tasks"> -n {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpiexec</executable>
      <arguments>
	<arg name="num_tasks"> -n {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="purge"></command>
      </modules>
      <modules compiler="intel">
	<command name="load">compiler/intel/20.0.1</command>
	<command name="load">tool/netcdf/4.7.4/intel/20.0.1</command>
      </modules>
      <modules compiler="intel" mpilib="mvapich2">
	<command name="load">mpi/2.3.3/intel/20.0.1</command>
      </modules>
      <modules compiler="pgi">
	<command name="load">compiler/pgi/20.1</command>
	<command name="load">tool/netcdf/4.7.4/pgi/20.1</command>
      </modules>
      <modules compiler="nag">
	<command name="load">compiler/nag/6.2-8.1.0</command>
	<command name="load">tool/netcdf/c4.6.1-f4.4.4/nag-gnu/6.2-8.1.0</command>
      </modules>
      <modules compiler="nag" mpilib="mvapich2">
	<command name="load">mpi/2.3.3/nag/6.2</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">compiler/gnu/9.3.0</command>
	<command name="load">tool/netcdf/4.7.4/gnu/9.3.0</command>
      </modules>
      <modules compiler="gnu" mpilib="openmpi">
	<command name="load">openmpi/4.0.3/gnu/9.3.0</command>
      </modules>
      <modules compiler="gnu" mpilib="mvapich2">
	<command name="load">mpi/2.3.3/gnu/9.3.0</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
      <!-- The following is needed in order to run qsub from the compute nodes -->
      <env name="PATH">$ENV{PATH}:/cluster/torque/bin</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="laramie">
    <DESC>NCAR SGI test platform, os is Linux, 36 pes/node, batch system is PBS</DESC>
    <NODENAME_REGEX>.*.laramie.ucar.edu</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,gnu</COMPILERS>
    <MPILIBS>mpt</MPILIBS>
    <CIME_OUTPUT_ROOT>/picnic/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{CESMDATAROOT}/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{CESMDATAROOT}/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESMDATAROOT}/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{CESMDATAROOT}/tools/cime/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <!-- have not seen any performance benefit in smt -->
    <MAX_TASKS_PER_NODE>36</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>36</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec_mpt</executable>
      <arguments>
	<arg name="labelstdout">-p "%g:"</arg>
	<arg name="threadplacement"> omplace </arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/picnic/u/apps/la/opt/lmod/7.5.3/gnu/4.8.5/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/picnic/u/apps/la/opt/lmod/7.5.3/gnu/4.8.5/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/picnic/u/apps/la/opt/lmod/7.5.3/gnu/4.8.5/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/picnic/u/apps/la/opt/lmod/7.5.3/gnu/4.8.5/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/picnic/u/apps/la/opt/lmod/7.5.3/gnu/4.8.5/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/picnic/u/apps/la/opt/lmod/7.5.3/gnu/4.8.5/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">ncarenv/1.2</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/17.0.1</command>
	<command name="load">mkl</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">gnu/6.3.0</command>
	<command name="load">openblas/0.2.14</command>
      </modules>
      <modules mpilib="mpt">
	<command name="load">mpt/2.17</command>
	<command name="load">netcdf-mpi/4.4.1.1</command>
      </modules>
      <modules mpilib="mpt" compiler="intel">
	<command name="load">pnetcdf/1.8.1</command>
      </modules>
      <modules mpilib="openmpi">
	<command name="load">openmpi/2.1.0</command>
	<command name="load">netcdf/4.4.1.1</command>
      </modules>
      <modules>
	<command name="load">ncarcompilers/0.4.1</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">netcdf/4.4.1.1</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="MPI_TYPE_DEPTH">16</env>
    </environment_variables>
  </machine>

  <machine MACH="lawrencium-lr3">
    <DESC>Lawrencium LR3 cluster at LBL, OS is Linux (intel), batch system is SLURM</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/global/scratch/$ENV{USER}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/global/scratch/$ENV{USER}/cesm_input_datasets/</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/global/scratch/$ENV{USER}/cesm_input_datasets/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/cesm_archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$CIME_OUTPUT_ROOT/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/$CIME_OUTPUT_ROOT/cesm_tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>rgknox at lbl dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>16</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
	<arg name="num_tasks">-np {{ total_tasks }}</arg>
	<arg name="tasks_per_node"> -npernode $MAX_MPITASKS_PER_NODE </arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <init_path lang="perl">/usr/Modules/init/perl.pm</init_path>
      <init_path lang="python">/usr/Modules/python.py</init_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="perl">/usr/Modules/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/Modules/bin/modulecmd python</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">cmake</command>
	<command name="load">perl xml-libxml switch python/2.7</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/2016.4.072</command>
	<command name="load">mkl</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial">
	<command name="load">netcdf/4.4.1.1-intel-s</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial">
	<command name="load">openmpi</command>
	<command name="load">netcdf/4.4.1.1-intel-p</command>
      </modules>
    </module_system>
  </machine>

  <machine MACH="lawrencium-lr2">
    <DESC>Lawrencium LR2 cluster at LBL, OS is Linux (intel), batch system is SLURM</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/global/scratch/$ENV{USER}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/global/scratch/$ENV{USER}/cesm_input_datasets/</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/global/scratch/$ENV{USER}/cesm_input_datasets/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/cesm_archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$CIME_OUTPUT_ROOT/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/$CIME_OUTPUT_ROOT/cesm_tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>rgknox and gbisht at lbl dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>12</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>12</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
	<arg name="num_tasks"> -np {{ total_tasks }}</arg>
	<arg name="tasks_per_node"> -npernode $MAX_MPITASKS_PER_NODE</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <init_path lang="perl">/usr/Modules/init/perl.pm</init_path>
      <init_path lang="python">/usr/Modules/python.py</init_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="perl">/usr/Modules/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/Modules/bin/modulecmd python</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">cmake</command>
	<command name="load">perl xml-libxml switch python/2.7</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/2016.4.072</command>
	<command name="load">mkl</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial">
	<command name="load">netcdf/4.4.1.1-intel-s</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial">
	<command name="load">openmpi</command>
	<command name="load">netcdf/4.4.1.1-intel-p</command>
      </modules>
    </module_system>
  </machine>

  <machine MACH="lonestar5">
    <DESC>Lonestar5 cluster at TACC, OS is Linux (intel), batch system is SLURM</DESC>
    <NODENAME_REGEX>.*ls5\.tacc\.utexas\.edu</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/work/02503/edwardsj/CESM/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/work/02503/edwardsj/CESM/inputdata/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/cesm_archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/work/02503/edwardsj/CESM/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/work/02503/edwardsj/CESM/cime/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>48</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>24</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
	<arg name="num_tasks">--ntasks={{ total_tasks }}</arg>
	<arg name="cpu-bind">--cpu-bind=v</arg>
      </arguments>
    </mpirun>
    <!-- allow ls5 modules to write to stderr without cime error -->
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/opt/apps/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/opt/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/opt/apps/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/opt/apps/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/opt/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/opt/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules>
	<command name="reset"/>
	<command name="load">cmake</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/18.0.2</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">netcdf/4.6.2</command>
      </modules>
      <modules mpilib="mpich">
	<command name="load">cray_mpich</command>
      </modules>
      <modules mpilib="!mpi-serial">
	<command name="load">pnetcdf/1.8.0</command>
	<command name="load">parallel-netcdf/4.6.2</command>
      </modules>
    </module_system>
  </machine>


  <machine MACH="melvin">
    <DESC>Linux workstation for Jenkins testing</DESC>
    <NODENAME_REGEX>(melvin|watson)</NODENAME_REGEX>
    <OS>LINUX</OS>
    <PROXY>sonproxy.sandia.gov:80</PROXY>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <SAVE_TIMING_DIR>/sems-data-store/ACME/timings</SAVE_TIMING_DIR>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/acme/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/sems-data-store/ACME/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/sems-data-store/ACME/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/sems-data-store/ACME/baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/sems-data-store/ACME/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>32</GMAKE_J>
    <TESTS>acme_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>64</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
	<arg name="num_tasks"> -np {{ total_tasks }}</arg>
	<arg name="tasks_per_node"> --map-by ppr:{{ tasks_per_numa }}:socket:PE=$ENV{OMP_NUM_THREADS} --bind-to hwthread</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">sems-env</command>
	<command name="load">acme-env</command>
	<command name="load">sems-git</command>
	<command name="load">sems-python/2.7.9</command>
	<command name="load">sems-cmake/2.8.12</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">sems-gcc/5.3.0</command>
      </modules>
      <modules compiler="intel">
	<command name="load">sems-intel/16.0.3</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">sems-netcdf/4.4.1/exo</command>
	<command name="load">acme-pfunit/3.2.8/base</command>
      </modules>
      <modules mpilib="!mpi-serial">
	<command name="load">sems-openmpi/1.8.7</command>
	<command name="load">sems-netcdf/4.4.1/exo_parallel</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="NETCDFROOT">$ENV{SEMS_NETCDF_ROOT}</env>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
    </environment_variables>
    <environment_variables mpilib="!mpi-serial">
      <env name="PNETCDFROOT">$ENV{SEMS_NETCDF_ROOT}</env>
    </environment_variables>
  </machine>

  <machine MACH="mira">
    <DESC>ANL IBM BG/Q, os is BGP, 16 pes/node, batch system is cobalt</DESC>
    <NODENAME_REGEX>.*.fst.alcf.anl.gov</NODENAME_REGEX>
    <OS>BGQ</OS>
    <COMPILERS>ibm</COMPILERS>
    <MPILIBS>ibm</MPILIBS>
    <CIME_OUTPUT_ROOT>/projects/$PROJECT/usr/$ENV{USER}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/projects/ccsm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/projects/ccsm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/projects/$PROJECT/usr/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/projects/ccsm/ccsm_baselines/</BASELINE_ROOT>
    <CCSM_CPRNC>/projects/ccsm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>cobalt</BATCH_SYSTEM>
    <SUPPORTED_BY> cseg </SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>8</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>/usr/bin/runjob</executable>
      <arguments>
	<arg name="label"> --label short</arg>
	<!-- Ranks per node!! -->
	<arg name="tasks_per_node"> --ranks-per-node $MAX_MPITASKS_PER_NODE</arg>
	<!-- Total MPI Tasks -->
	<arg name="num_tasks"> --np {{ total_tasks }}</arg>
	<arg name="locargs">--block $COBALT_PARTNAME --envs OMP_WAIT_POLICY=active --envs BG_SMP_FAST_WAKEUP=yes $LOCARGS</arg>
	<arg name="bg_threadlayout"> --envs BG_THREADLAYOUT=1</arg>
	<arg name="omp_stacksize"> --envs OMP_STACKSIZE=32M</arg>
	<arg name="thread_count"> --envs OMP_NUM_THREADS=$ENV{OMP_NUM_THREADS}</arg>
      </arguments>
    </mpirun>
    <module_system type="soft">
      <init_path lang="csh">/etc/profile.d/00softenv.csh</init_path>
      <init_path lang="sh">/etc/profile.d/00softenv.sh</init_path>
      <cmd_path lang="csh">soft</cmd_path>
      <cmd_path lang="sh">soft</cmd_path>
      <modules>
	<command name="add">+mpiwrapper-xl</command>
	<command name="add">@ibm-compilers-2015-02</command>
	<command name="add">+cmake</command>
	<command name="add">+python</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="MPI_TYPE_MAX">10000</env>
      <env name="OMP_DYNAMIC">FALSE</env>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="HDF5">/soft/libraries/hdf5/1.8.14/cnk-xl/current</env>
    </environment_variables>
  </machine>

  <machine MACH="modex">
      <DESC>Medium sized linux cluster at BNL, torque scheduler.</DESC>
      <OS>LINUX</OS>
      <COMPILERS>gnu</COMPILERS>
      <MPILIBS>openmpi,mpi-serial</MPILIBS>
      <CIME_OUTPUT_ROOT>/data/$ENV{USER}</CIME_OUTPUT_ROOT>
      <DIN_LOC_ROOT>/data/Model_Data/cesm_input_datasets/</DIN_LOC_ROOT>
      <DIN_LOC_ROOT_CLMFORC>/data/Model_Data/cesm_input_datasets/atm/datm7</DIN_LOC_ROOT_CLMFORC>
      <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/cesm_archive/$CASE</DOUT_S_ROOT>
      <BASELINE_ROOT>$CIME_OUTPUT_ROOT/cesm_baselines</BASELINE_ROOT>
      <CCSM_CPRNC>/data/software/cesm_tools/cprnc/cprnc</CCSM_CPRNC>
      <GMAKE_J>4</GMAKE_J>
      <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
      <SUPPORTED_BY>rgknox at lbl dot gov and sserbin at bnl gov</SUPPORTED_BY>
      <MAX_TASKS_PER_NODE>12</MAX_TASKS_PER_NODE>
      <MAX_MPITASKS_PER_NODE>12</MAX_MPITASKS_PER_NODE>
      <COSTPES_PER_NODE>12</COSTPES_PER_NODE>
      <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
      <mpirun mpilib="default">
	  <executable>mpirun</executable>
	  <arguments>
	      <arg name="num_tasks">-np {{ total_tasks }}</arg>
	      <arg name="tasks_per_node">-npernode $MAX_TASKS_PER_NODE</arg>
	  </arguments>
      </mpirun>
      <module_system type="module">
	  <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
	  <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
	  <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
	  <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
	  <cmd_path lang="sh">module</cmd_path>
	  <cmd_path lang="csh">module</cmd_path>
	  <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
	  <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
	  <modules>
	      <command name="purge"/>
	      <command name="load">perl/5.22.1</command>
	      <command name="load">libxml2/2.9.2</command>
	      <command name="load">maui/3.3.1</command>
	      <command name="load">python/2.7.13</command>
	  </modules>
	  <modules compiler="gnu">
	      <command name="load">gcc/5.4.0</command>
	      <command name="load">gfortran/5.4.0</command>
	      <command name="load">hdf5/1.8.19fates</command>
	      <command name="load">netcdf/4.4.1.1-gnu540-fates</command>
	      <command name="load">openmpi/2.1.1-gnu540</command>
	  </modules>
	  <modules compiler="gnu" mpilib="!mpi-serial">
	      <command name="load">openmpi/2.1.1-gnu540</command>
	  </modules>
      </module_system>
       <environment_variables>
	 <env name="HDF5_HOME">/data/software/hdf5/1.8.19fates</env>
	 <env name="NETCDF_PATH">/data/software/netcdf/4.4.1.1-gnu540-fates</env>
       </environment_variables>
  </machine>

  <machine MACH="olympus">
    <DESC>PNL cluster, os is Linux (pgi), batch system is SLURM</DESC>
    <OS>LINUX</OS>
    <COMPILERS>pgi</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>/pic/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/pic/scratch/tcraig/IRESM/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/pic/scratch/tcraig/IRESM/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/pic/scratch/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/pic/scratch/tcraig/IRESM/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/pic/scratch/tcraig/IRESM/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>tcraig -at- ucar.edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec_mpt</executable>
      <arguments>
	<arg name="mpi">--mpi=none</arg>
	<arg name="num_tasks">-n={{ total_tasks }}</arg>
	<arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/share/apps/modules/Modules/3.2.7/init/perl.pm</init_path>
      <init_path lang="csh">/share/apps/modules/Modules/3.2.7/init/csh</init_path>
      <init_path lang="sh">/share/apps/modules/Modules/3.2.7/init/sh</init_path>
      <cmd_path lang="perl">/share/apps/modules/Modules/3.2.7/bin/modulecmd perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">precision/i4</command>
	<command name="load">pgi/11.8</command>
	<command name="load">mvapich2/1.7</command>
	<command name="load">netcdf/4.1.3</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
  </machine>

  <machine MACH="pleiades-bro">
    <DESC>NASA/AMES Linux Cluster, Linux (ia64), 2.4 GHz Broadwell Intel Xeon E5-2680v4 processors, 28 pes/node (two 14-core processors) and 128 GB of memory/node, batch system is PBS</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>mpt</MPILIBS>
    <CIME_OUTPUT_ROOT>/nobackup/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/nobackup/fvitt/csm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/nobackup/fvitt/csm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/nobackup/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/nobackup/fvitt/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/u/fvitt/bin/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>fvitt -at- ucar.edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>28</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>28</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec_mpt</executable>
      <arguments>
	<arg name="num_tasks">-n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/usr/share/Modules/3.2.10/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/Modules/3.2.10/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/3.2.10/init/csh</init_path>
      <init_path lang="python">/usr/share/Modules/3.2.10/init/python.py</init_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">nas</command>
	<command name="load">pkgsrc</command>
	<command name="load">comp-intel/2018.3.222</command>
	<command name="load">mpi-sgi/mpt.2.15r20</command>
	<command name="load">szip/2.1.1</command>
	<command name="load">hdf4/4.2.12</command>
	<command name="load">hdf5/1.8.18_mpt</command>
	<command name="load">netcdf/4.4.1.1_mpt</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="MPI_GROUP_MAX">1024</env>
      <env name="MPI_TYPE_MAX">100000</env>
      <env name="MPI_TYPE_DEPTH">16</env>
      <env name="KMP_AFFINITY">noverbose,disabled</env>
      <env name="KMP_SCHEDULE">static,balanced</env>
      <env name="OMP_STACKSIZE">256M</env>
    </environment_variables>
  </machine>

  <machine MACH="pleiades-has">
    <DESC>NASA/AMES Linux Cluster, Linux (ia64), 2.5 GHz Haswell Intel Xeon E5-2680v3 processors, 24 pes/node (two 12-core processors) and 128 GB of memory/node, batch system is PBS</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>mpt</MPILIBS>
    <CIME_OUTPUT_ROOT>/nobackup/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/nobackup/fvitt/csm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/nobackup/fvitt/csm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/nobackup/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/nobackup/fvitt/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/u/fvitt/bin/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>fvitt -at- ucar.edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>24</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec_mpt</executable>
      <arguments>
	<arg name="num_tasks">-n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/usr/share/Modules/3.2.10/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/Modules/3.2.10/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/3.2.10/init/csh</init_path>
      <init_path lang="python">/usr/share/Modules/3.2.10/init/python.py</init_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">nas</command>
	<command name="load">pkgsrc</command>
	<command name="load">comp-intel/2018.3.222</command>
	<command name="load">mpi-sgi/mpt.2.15r20</command>
	<command name="load">szip/2.1.1</command>
	<command name="load">hdf4/4.2.12</command>
	<command name="load">hdf5/1.8.18_mpt</command>
	<command name="load">netcdf/4.4.1.1_mpt</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="MPI_GROUP_MAX">1024</env>
      <env name="MPI_TYPE_MAX">100000</env>
      <env name="MPI_TYPE_DEPTH">16</env>
      <env name="KMP_AFFINITY">noverbose,disabled</env>
      <env name="KMP_SCHEDULE">static,balanced</env>
      <env name="OMP_STACKSIZE">256M</env>
    </environment_variables>
  </machine>

  <machine MACH="pleiades-san">
    <DESC>NASA/AMES Linux Cluster, Linux (ia64), Altix ICE, 2.6 GHz Sandy Bridge processors, 16 cores/node and 32 GB of memory, batch system is PBS</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>mpt</MPILIBS>
    <CIME_OUTPUT_ROOT>/nobackup/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/nobackup/fvitt/csm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/nobackup/fvitt/csm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/nobackup/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/nobackup/fvitt/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/u/fvitt/bin/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>fvitt -at- ucar.edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>16</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec_mpt</executable>
      <arguments>
	<arg name="num_tasks">-n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/usr/share/Modules/3.2.10/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/Modules/3.2.10/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/3.2.10/init/csh</init_path>
      <init_path lang="python">/usr/share/Modules/3.2.10/init/python.py</init_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">nas</command>
	<command name="load">pkgsrc</command>
	<command name="load">comp-intel/2018.3.222</command>
	<command name="load">mpi-sgi/mpt.2.15r20</command>
	<command name="load">szip/2.1.1</command>
	<command name="load">hdf4/4.2.12</command>
	<command name="load">hdf5/1.8.18_mpt</command>
	<command name="load">netcdf/4.4.1.1_mpt</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="MPI_GROUP_MAX">1024</env>
      <env name="MPI_TYPE_MAX">100000</env>
      <env name="MPI_TYPE_DEPTH">16</env>
      <env name="KMP_AFFINITY">noverbose,disabled</env>
      <env name="KMP_SCHEDULE">static,balanced</env>
      <env name="OMP_STACKSIZE">256M</env>
    </environment_variables>
  </machine>

  <machine MACH="pleiades-ivy">
    <DESC>NASA/AMES Linux Cluster, Linux (ia64), Altix ICE, 2.8 GHz Ivy Bridge processors, 20 cores/node and 3.2 GB of memory per core, batch system is PBS</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>/nobackup/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/nobackup/fvitt/csm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/nobackup/fvitt/csm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/nobackup/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/nobackup/fvitt/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/u/fvitt/bin/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>fvitt -at- ucar.edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>20</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>20</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec_mpt</executable>
      <arguments>
	<arg name="num_tasks">-n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/usr/share/Modules/3.2.10/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/Modules/3.2.10/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/3.2.10/init/csh</init_path>
      <init_path lang="python">/usr/share/Modules/3.2.10/init/python.py</init_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">nas</command>
	<command name="load">pkgsrc</command>
	<command name="load">comp-intel/2018.3.222</command>
	<command name="load">mpi-sgi/mpt.2.15r20</command>
	<command name="load">szip/2.1.1</command>
	<command name="load">hdf4/4.2.12</command>
	<command name="load">hdf5/1.8.18_mpt</command>
	<command name="load">netcdf/4.4.1.1_mpt</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="MPI_GROUP_MAX">1024</env>
      <env name="MPI_TYPE_MAX">100000</env>
      <env name="MPI_TYPE_DEPTH">16</env>
      <env name="KMP_AFFINITY">noverbose,disabled</env>
      <env name="KMP_SCHEDULE">static,balanced</env>
      <env name="OMP_STACKSIZE">256M</env>
    </environment_variables>
  </machine>

  <machine MACH="sandia-srn-sems">
    <DESC>Linux workstation at Sandia on SRN with SEMS TPL modules</DESC>
    <NODENAME_REGEX>(s999964|climate|penn)</NODENAME_REGEX>
    <OS>LINUX</OS>
    <PROXY>wwwproxy.sandia.gov:80</PROXY>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <SAVE_TIMING_DIR>/sems-data-store/ACME/timings</SAVE_TIMING_DIR>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/acme/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/sems-data-store/ACME/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/sems-data-store/ACME/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/sems-data-store/ACME/baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/sems-data-store/ACME/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>32</GMAKE_J>
    <TESTS>acme_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>64</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
	<arg name="num_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">sems-env</command>
	<command name="load">sems-git</command>
	<command name="load">sems-python/2.7.9</command>
	<command name="load">sems-gcc/5.1.0</command>
	<command name="load">sems-openmpi/1.8.7</command>
	<command name="load">sems-cmake/2.8.12</command>
	<command name="load">sems-netcdf/4.3.2/parallel</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="NETCDFROOT">$ENV{SEMS_NETCDF_ROOT}</env>
      <env name="PNETCDFROOT">$ENV{SEMS_NETCDF_ROOT}</env>
    </environment_variables>
  </machine>

  <machine MACH="sandiatoss3">
    <DESC>SNL clust</DESC>
    <NODENAME_REGEX>(skybridge|chama)-login</NODENAME_REGEX>
    <OS>LINUX</OS>
    <PROXY>wwwproxy.sandia.gov:80</PROXY>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <SAVE_TIMING_DIR>/projects/ccsm/timings</SAVE_TIMING_DIR>
    <CIME_OUTPUT_ROOT>/gscratch/$USER/acme_scratch/$MACH</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/projects/ccsm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/projects/ccsm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>               <!-- complete path to a short term archiving directory -->
    <BASELINE_ROOT>/projects/ccsm/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/projects/ccsm/cprnc/build/cprnc_wrap</CCSM_CPRNC>                <!-- path to the cprnc tool used to compare netcdf history files in testing -->
    <GMAKE_J>8</GMAKE_J>
    <TESTS>acme_integration</TESTS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>16</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>

    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
	<arg name="num_tasks"> -np {{ total_tasks }}</arg>
	<arg name="tasks_per_node"> -npernode $MAX_MPITASKS_PER_NODE</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">sems-env</command>
	<command name="load">sems-git</command>
	<command name="load">sems-python/2.7.9</command>
	<command name="load">gnu/4.9.2</command>
	<command name="load">intel/intel-15.0.3.187</command>
	<command name="load">libraries/intel-mkl-15.0.2.164</command>
	<command name="load">libraries/intel-mkl-15.0.2.164</command>
      </modules>
      <modules mpilib="!mpi-serial">
	<command name="load" >openmpi-intel/1.8</command>
	<command name="load" >sems-hdf5/1.8.12/parallel</command>
	<command name="load" >sems-netcdf/4.3.2/parallel</command>
	<command name="load" >sems-hdf5/1.8.12/base</command>
	<command name="load" >sems-netcdf/4.3.2/base</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="NETCDFROOT">$ENV{SEMS_NETCDF_ROOT}</env>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <environment_variables mpilib="!mpi-serial">
      <env name="PNETCDFROOT">$ENV{SEMS_NETCDF_ROOT}</env>
    </environment_variables>
  </machine>

  <machine MACH="stampede3-spr">
    <DESC>Intel Xeon CPU MAX 9480 ("Sapphire Rapids HBM") 112 cores on two sockets (2x 56 cores), batch system is SLURM</DESC>
    <NODENAME_REGEX>.*.stampede3.tacc.utexas.edu</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi,mvapich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/work/02503/edwardsj/CESM/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/work/02503/edwardsj/CESM/inputdata/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{WORK}/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/work/02503/edwardsj/CESM/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/work/02503/edwardsj/CESM/cime/tools/cprnc/cprnc.sp3</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>112</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>112</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="impi">
      <executable>ibrun</executable>
      <arguments>
        <arg name="ntasks"> -n {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mvapich2">
      <executable>ibrun</executable>
      <arguments>
        <arg name="ntasks"> -n {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/apps/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/opt/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/opt/apps/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/opt/apps/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/opt/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/opt/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"></command>
        <command name="load">TACC</command>
        <command name="load">intel/24.0</command>
        <command name="load">cmake/3.28.1</command>
      </modules>
      <modules mpilib="mvapich2">
        <command name="load">mvapich/3.0</command>
      </modules>
      <modules mpilib="impi">
        <command name="load">impi</command>
      </modules>
      <modules mpilib="!mpi-serial">	
        <command name="load">pnetcdf/1.12.3</command>
        <command name="load">parallel-netcdf/4.9.2</command>
	<command name="load">phdf5/1.14.3</command>
      </modules>  
      <modules mpilib="mpi-serial">
        <command name="load">netcdf/4.9.2</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
     </environment_variables>
    <environment_variables mpilib="impi">
      <env name="I_MPI_F90">ifort</env>
    </environment_variables>
  </machine>



  <machine MACH="stampede2-skx">
    <DESC>Intel Xeon Platinum 8160 ("Skylake"),48 cores on two sockets (24 cores/socket) , batch system is SLURM</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi,mvapich2</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/work/02503/edwardsj/CESM/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/work/02503/edwardsj/CESM/inputdata/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{WORK}/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/work/02503/edwardsj/CESM/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/work/02503/edwardsj/CESM/cime/tools/cprnc/cprnc.sp3</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>96</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>48</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="impi">
      <executable>ibrun</executable>
      <arguments>
        <arg name="ntasks"> -n {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mvapich2">
      <executable>ibrun</executable>
      <arguments>
        <arg name="ntasks"> -n {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
        <module_system type="module">
      <init_path lang="perl">/opt/apps/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/opt/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/opt/apps/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/opt/apps/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/opt/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/opt/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"></command>
        <command name="load">TACC</command>
        <command name="load">intel/24.0</command>
        <command name="load">cmake/3.28.1</command>
      </modules>
      <modules mpilib="mvapich2">
        <command name="load">mvapich/3.0</command>
      </modules>
      <modules mpilib="impi">
        <command name="load">impi</command>
      </modules>
      <modules mpilib="!mpi-serial">	
        <command name="load">pnetcdf/1.12.3</command>
        <command name="load">parallel-netcdf/4.9.2</command>
	<command name="load">phdf5/1.14.3</command>
      </modules>  
      <modules mpilib="mpi-serial">
        <command name="load">netcdf/4.9.2</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
     </environment_variables>
    <environment_variables comp_interface="nuopc" mpilib="impi">
      <env name="ESMFMKFILE">/work2/02503/edwardsj/stampede3/intel24.0/esmf/v8.6.1b04/lib/libO/Linux.intel.64.intelmpi.default/esmf.mk</env> 
      <env name="ESMF_RUNTIME_PROFILE">ON</env>
      <env name="ESMF_RUNTIME_PROFILE_OUTPUT">SUMMARY</env>
      <env name="UGCSINPUTPATH">/work/06242/tg855414/stampede2/FV3GFS/benchmark-inputs/2012010100/gfs/fcst</env>
      <env name="UGCSFIXEDFILEPATH">/work/06242/tg855414/stampede2/FV3GFS/fix_am</env>
      <env name="UGCSADDONPATH">/work/06242/tg855414/stampede2/FV3GFS/addon</env>
      <env name="I_MPI_F90">ifort</env>
    </environment_variables>
  </machine>

  <machine MACH="stampede2-knl">
    <DESC>Intel Xeon Phi 7250 ("Knights Landing") , batch system is SLURM</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi,mvapich2</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/work/02503/edwardsj/CESM/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/work/02503/edwardsj/CESM/inputdata/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{WORK}/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/work/02503/edwardsj/CESM/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/work/02503/edwardsj/CESM/cime/tools/cprnc/cprnc.sp3</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>256</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>64</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="impi">
      <executable>ibrun</executable>
    </mpirun>
    <mpirun mpilib="mvapich2">
      <executable>ibrun</executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/apps/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/opt/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/opt/apps/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/opt/apps/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/opt/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/opt/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"></command>
	<command name="load">TACC</command>
	<command name="load">python2/2.7.15</command>
	<command name="load">intel/17.0.4</command>
	<command name="load">cmake/3.20.2</command>
      </modules>
      <modules mpilib="mvapich2">
	<command name="load">mvapich2/2.3b</command>
	<command name="load">pnetcdf/1.8.1</command>
	<command name="load">parallel-netcdf/4.3.3.1</command>
      </modules>
      <modules mpilib="impi">
	<command name="rm">mvapich2</command>
	<command name="load">impi/17.0.3</command>
	<command name="load">pnetcdf/1.8.1</command>
	<command name="load">parallel-netcdf/4.3.3.1</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">netcdf/4.3.3.1</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
    </environment_variables>
  </machine>

  <machine MACH="theia">
    <DESC>theia</DESC>
    <NODENAME_REGEX>tfe</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi</MPILIBS>
    <PROJECT>nems</PROJECT>
    <SAVE_TIMING_DIR/>
    <CIME_OUTPUT_ROOT>/scratch4/NCEPDEV/nems/noscrub/$USER/cimecases</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/scratch4/NCEPDEV/nems/noscrub/Rocky.Dunlap/cesmdataroot/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/scratch4/NCEPDEV/nems/noscrub/Rocky.Dunlap/cesmdataroot/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/scratch4/NCEPDEV/nems/noscrub/Rocky.Dunlap/BASELINES</BASELINE_ROOT>
    <CCSM_CPRNC>/scratch4/NCEPDEV/nems/noscrub/Rocky.Dunlap/cesmdataroot/tools/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>24</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
	<arg name="num_tasks">-n $TOTALPES</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpi-serial">
      <executable></executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="sh">/apps/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/apps/lmod/lmod/init/csh</init_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="python">/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <modules compiler="intel">
	<command name="purge"/>
	<command name="load">intel</command>
	<command name="load">impi/5.1.1.109</command>
	<command name="load">netcdf/4.3.0</command>
	<command name="load">pnetcdf</command>
	<command name="use">/scratch4/NCEPDEV/nems/noscrub/emc.nemspara/soft/modulefiles</command>
	<command name="load">esmf/8.0.0bs15</command>
      </modules>
    </module_system>
  </machine>

  <machine MACH="theta">
    <DESC>ALCF Cray XC* KNL, os is CNL, 64 pes/node, batch system is cobalt</DESC>
    <NODENAME_REGEX>theta.*</NODENAME_REGEX>
    <OS>CNL</OS>
    <COMPILERS>intel,gnu,cray</COMPILERS>
    <MPILIBS>mpt</MPILIBS>
    <PROJECT>CESM_Highres_Testing</PROJECT>
    <CIME_OUTPUT_ROOT>/projects/CESM_Highres_Testing/cesm/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/projects/CESM_Highres_Testing/cesm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/projects/CESM_Highres_Testing/cesm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/projects/CESM_Highres_Testing/cesm/baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/projects/CESM_Highres_Testing/cesm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>cobalt_theta</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>64</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>aprun</executable>
      <arguments>
	<arg name="num_tasks" >-n {{ total_tasks }}</arg>
	<arg name="tasks_per_node" >-N {{ tasks_per_node }} </arg>
	<arg name="thread_count">--cc depth -d $OMP_NUM_THREADS</arg>
	<arg name="env_omp_stacksize">-e OMP_STACKSIZE=64M</arg>
	<arg name="env_thread_count">-e OMP_NUM_THREADS=$OMP_NUM_THREADS</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/modules/default/init/perl.pm</init_path>
      <init_path lang="python">/opt/modules/default/init/python.py</init_path>
      <init_path lang="sh">/opt/modules/default/init/sh</init_path>
      <init_path lang="csh">/opt/modules/default/init/csh</init_path>
      <cmd_path lang="perl">/opt/modules/default/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/opt/modules/default/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="rm">craype-mic-knl</command>
	<command name="rm">PrgEnv-intel</command>
	<command name="rm">PrgEnv-cray</command>
	<command name="rm">PrgEnv-gnu</command>
	<command name="rm">intel</command>
	<command name="rm">cce</command>
	<command name="rm">cray-parallel-netcdf</command>
	<command name="rm">cray-hdf5-parallel</command>
	<command name="rm">pmi</command>
	<command name="rm">cray-libsci</command>
	<command name="rm">cray-mpich</command>
	<command name="rm">cray-netcdf</command>
	<command name="rm">cray-hdf5</command>
	<command name="rm">cray-netcdf-hdf5parallel</command>
	<command name="rm">craype</command>
	<command name="rm">papi</command>
      </modules>

      <modules compiler="intel">
	<command name="load">PrgEnv-intel/6.0.4</command>
	<command name="switch">intel intel/18.0.0.128</command>
	<command name="rm">cray-libsci</command>
      </modules>

      <modules compiler="cray">
	<command name="load">PrgEnv-cray/6.0.4</command>
	<command name="switch">cce cce/8.7.0</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">PrgEnv-gnu/6.0.4</command>
	<command name="switch">gcc gcc/7.3.0</command>
      </modules>
      <modules>
	<command name="load">papi/5.6.0.1</command>
	<command name="swap">craype craype/2.5.14</command>
      </modules>
      <modules compiler="!intel">
	<command name="switch">cray-libsci/18.04.1</command>
      </modules>
      <modules>
	<command name="load">cray-mpich/7.7.0</command>
      </modules>
      <modules mpilib="mpt">
	<command name="load">cray-netcdf-hdf5parallel/4.4.1.1.6</command>
	<command name="load">cray-hdf5-parallel/1.10.1.1</command>
	<command name="load">cray-parallel-netcdf/1.8.1.3</command>
      </modules>
    </module_system>
  </machine>

  <machine MACH="vista">
    <DESC>Grace Hopper ARM TACC , batch system is SLURM</DESC>
    <NODENAME_REGEX>.*.vista.tacc.utexas.edu</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>nvhpc,gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/work2/02503/edwardsj/CESM/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/work2/02503/edwardsj/CESM/inputdata/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/work2/02503/edwardsj/CESM/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/work2/02503/edwardsj/CESM/cime/tools/cprnc/cprnc.vista</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY></SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>144</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>144</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
	<arg name="num_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/apps/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/opt/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/opt/apps/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/opt/apps/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/opt/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/opt/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"></command>
	<command name="load">TACC</command>
	<command name="load">nvidia</command>
      </modules>
      <modules mpilib="openmpi">
	<command name="load">openmpi/5.0.3</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="NETCDF_PATH">/scratch/00422/cazes/vista_netcdf_install/netcdf_4.9.2_nvhpc/</env>
      <env name="PNETCDF_PATH">/scratch/00422/cazes/vista_netcdf_install/pnetcdf_1.12.3_nvhpc/</env>
    </environment_variables>
  </machine>


  <machine MACH="zeus">
    <DESC> CMCC Lenovo ThinkSystem SD530, os is Linux, 36 pes/node, batch system is LSF</DESC>
    <NODENAME_REGEX>(login[1,2]-ib|n[0-9][0-9][0-9]-ib)</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi,mpi-serial</MPILIBS>
    <CIME_OUTPUT_ROOT>/work/$ENV{DIVISION}/$ENV{USER}/CESM2</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{CESMDATAROOT}/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$DIN_LOC_ROOT/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESMDATAROOT}/ccsm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{CESMDATAROOT}/cesm2_tools/cprnc/cprnc</CCSM_CPRNC>
    <PERL5LIB>/usr/lib64/perl5:/usr/share/perl5</PERL5LIB>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>lsf</BATCH_SYSTEM>
    <SUPPORTED_BY>cmcc</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>72</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>36</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable> mpirun </executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python </cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
      </modules>
      <modules compiler="intel">
	<command name="load">intel19.5/19.5.281</command>
	<command name="load">intel19.5/szip/2.1.1</command>
	<command name="load">cmake/3.15.0</command>
	<command name="load">curl/7.66.0</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">intel19.5/hdf5/1.10.5</command>
	<command name="load">intel19.5/netcdf/C_4.7.2-F_4.5.2_CXX_4.3.1</command>
      </modules>
      <modules mpilib="!mpi-serial">
	<command name="load">impi19.5/19.5.281</command>
	<command name="load">impi19.5/hdf5/1.10.5</command>
	<command name="load">impi19.5/netcdf/C_4.7.2-F_4.5.2_CXX_4.3.1</command>
	<command name="load">impi19.5/parallel-netcdf/1.12.0</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial" DEBUG="TRUE">
	<command name="load">impi19.5/esmf/8.0.0-intelmpi-64-g</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial" DEBUG="FALSE">
	<command name="load">impi19.5/esmf/8.0.0-intelmpi-64-O</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial" DEBUG="TRUE">
	<command name="load">intel19.5/esmf/8.0.0-mpiuni-64-g</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial" DEBUG="FALSE">
	<command name="load">intel19.5/esmf/8.0.0-mpiuni-64-O</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="XIOS_PATH">/work/csp/cmip01/csm/xios</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="I_MPI_EXTRA_FILESYSTEM">1</env>
      <env name="I_MPI_EXTRA_FILESYSTEM_FORCE">gpfs</env>
      <env name="GPFSMPIO_TUNEBLOCKING">0</env>
      <env name="I_MPI_DEBUG">60</env>
      <env name="I_MPI_PLATFORM">skx</env>
      <env name="I_MPI_SHM">skx_avx512</env>
      <env name="I_MPI_HYDRA_BOOTSTRAP">lsf</env>
      <env name="I_MPI_LSF_USE_COLLECTIVE_LAUNCH">1</env>
    </environment_variables>
  </machine>
  <machine MACH="ubuntu-latest">
    <DESC>
      used for github testing
    </DESC>
    <NODENAME_REGEX/>
    <OS>LINUX</OS>
    <PROXY/>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <PROJECT>none</PROJECT>
    <SAVE_TIMING_DIR> </SAVE_TIMING_DIR>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/cesm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/cesm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/cesm/inputdata/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{HOME}/cesm/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{HOME}/cesm/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC/>
    <GMAKE>make</GMAKE>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>jedwards</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>4</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>4</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="openmpi">
      <executable>mpiexec</executable>
      <arguments>
	<arg name="ntasks"> -n {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <module_system type="none" />
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <default_run_suffix>
    <default_run_exe>${EXEROOT}/cesm.exe </default_run_exe>
    <default_run_misc_suffix> >> cesm.log.$LID 2>&amp;1 </default_run_misc_suffix>
  </default_run_suffix>

</config_machines>
