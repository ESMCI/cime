<?xml version="1.0"?>

<!--

===============================================================
COMPILER and COMPILERS
===============================================================
If a machine supports multiple compilers - then
- the settings for COMPILERS should reflect the supported compilers
as a comma separated string
- the setting for COMPILER should be the default compiler
(which is one of the values in COMPILERS)

===============================================================
MPILIB and MPILIBS
===============================================================
If a machine supports only one MPILIB is supported - then
the setting for  MPILIB and MPILIBS should be blank ("")
If a machine supports multiple mpi libraries (e.g. mpich and openmpi)
- the settings for MPILIBS should reflect the supported mpi libraries
as a comma separated string

The default settings for COMPILERS and MPILIBS is blank (in config_machines.xml)

Normally variable substitutions are not made until the case scripts are run, however variables
of the form $ENV{VARIABLE_NAME} are substituted in create_newcase from the environment
variable of the same name if it exists.

===============================================================
PROJECT_REQUIRED
===============================================================
A machine may need the PROJECT xml variable to be defined either because it is
used in some paths, or because it is used to give an account number in the job
submission script. If either of these are the case, then PROJECT_REQUIRED
should be set to TRUE for the given machine.


mpirun: the mpirun command that will be used to actually launch the model.
The attributes used to choose the mpirun command are:

mpilib: can either be 'default' the name of an mpi library, or a compiler name so one can choose the mpirun
based on the mpi library in use.

the 'executable' tag must have arguments required for the chosen mpirun, as well as the executable name.

unit_testing: can be 'true' or 'false'.
This allows using a different mpirun command to launch unit tests

-->

<config_machines version="2.0">
  <machine MACH="cheyenne">
    <DESC>NCAR SGI platform, os is Linux, 36 pes/node, batch system is PBS</DESC>
    <NODENAME_REGEX>.*.?cheyenne\d?.ucar.edu</NODENAME_REGEX>
    <!-- MPT sometimes timesout at model start time, the next two lines cause
    case_run.py to detect the timeout and retry FORCE_SPARE_NODES times -->
    <MPIRUN_RETRY_REGEX>MPT: Launcher network accept (MPI_LAUNCH_TIMEOUT) timed out</MPIRUN_RETRY_REGEX>
    <MPIRUN_RETRY_COUNT>10</MPIRUN_RETRY_COUNT>
    <OS>LINUX</OS>
    <COMPILERS>intel,gnu,pgi</COMPILERS>
    <MPILIBS compiler="intel" >mpt,openmpi</MPILIBS>
    <MPILIBS compiler="pgi" >openmpi,mpt</MPILIBS>
    <MPILIBS compiler="gnu" >mpt,openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/glade/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{CESMDATAROOT}/ufs_inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/glade/p/cgd/tss/CTSM_datm_forcing_data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESMDATAROOT}/ufs_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{CESMDATAROOT}/tools/cime/tools/cprnc/cprnc.cheyenne</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <!-- have not seen any performance benefit in smt -->
    <MAX_TASKS_PER_NODE>36</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>36</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec_mpt</executable>
      <arguments>
	<arg name="labelstdout">-p "%g:"</arg>
	<arg name="num_tasks"> -np {{ total_tasks }}</arg>
	<!-- the omplace argument needs to be last -->
	<arg name="zthreadplacement"> omplace -tm open64 </arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpt" queue="share">
      <executable>mpirun `hostname`</executable>
      <arguments>
	<arg name="anum_tasks"> -np {{ total_tasks }}</arg>
	<!-- the omplace argument needs to be last -->
	<arg name="zthreadplacement"> omplace -tm open64 </arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
	<arg name="anum_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="default" unit_testing="true">
      <!-- The only place we can build and run the unit tests is on cheyenne's
	   shared nodes. However, running mpi jobs on the shared nodes currently
	   requires some workarounds; these workarounds are implemented here -->
      <executable>/opt/sgi/mpt/mpt-2.15/bin/mpirun $ENV{UNIT_TEST_HOST} -np 1 </executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/glade/u/apps/ch/opt/lmod/7.5.3/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">ncarenv/1.3</command>
	<command name="load">cmake</command>
      </modules>
      <modules compiler="intel">
	<command name="load">intel/19.0.5</command>
	<command name="load">mkl</command>
      </modules>
      <modules compiler="pgi">
	<command name="load">pgi/19.3</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">gnu/8.3.0</command>
        <command name="load">openblas/0.3.6</command>
      </modules>
      <modules mpilib="mpt" compiler="gnu">
	<command name="load">mpt/2.19</command>
	<command name="load">netcdf/4.7.3</command>
      </modules>
      <modules mpilib="mpt" compiler="intel">
	<command name="load">mpt/2.19</command>
	<!-- known failure in parallel netcdf with mpt, use serial -->
	<command name="load">netcdf/4.7.3</command>
	<command name="load">pnetcdf/1.11.1</command>
      </modules>
      <modules mpilib="mpt" compiler="pgi">
	<command name="load">mpt/2.19</command>
	<command name="load">netcdf-mpi/4.7.1</command>
	<command name="load">pnetcdf/1.11.1</command>
      </modules>
      <modules mpilib="openmpi" compiler="pgi">
	<command name="load">openmpi/3.1.4</command>
	<command name="load">netcdf/4.7.1</command>
      </modules>
      <modules mpilib="openmpi" compiler="gnu">
        <command name="load">openmpi/3.1.4</command>
        <command name="load">netcdf/4.7.1</command>
      </modules>
      <modules>
	<command name="load">ncarcompilers/0.5.0</command>
      </modules>
      <modules mpilib="mpt" compiler="gnu">
	<command name="use">/glade/p/ral/jntp/GMTB/tools/modulefiles/gnu-8.3.0/mpt-2.19</command>
	<command name="load">NCEPlibs/1.0.0beta02</command>
      </modules>
      <modules mpilib="mpt" compiler="intel">
	<command name="use">/glade/p/ral/jntp/GMTB/tools/modulefiles/intel-19.0.5/mpt-2.19</command>
	<command name="load">NCEPlibs/1.0.0beta02</command>
      </modules>
      <modules compiler="gnu" mpilib="mpi-serial">
	<command name="load">netcdf/4.7.1</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial">
	<command name="load">netcdf/4.7.1</command>
      </modules>
      <modules compiler="pgi" mpilib="mpi-serial">
	<command name="load">netcdf/4.7.1</command>
      </modules>
    </module_system>
    <environment_variables comp_interface="nuopc">
      <env name="ESMF_RUNTIME_PROFILE">ON</env>
      <env name="ESMF_RUNTIME_PROFILE_OUTPUT">SUMMARY</env>
    </environment_variables>
    <environment_variables unit_testing="true">
      <env name="MPI_USE_ARRAY">false</env>
    </environment_variables>
    <environment_variables queue="share">
      <env name="MPI_USE_ARRAY">false</env>
    </environment_variables>
    <environment_variables>
      <env name="OMP_NUM_THREADS">1</env>
      <env name="OMP_STACKSIZE">1024M</env>
      <env name="MPI_TYPE_DEPTH">16</env>
      <env name="MPI_IB_CONGESTED">1</env>
      <env name="MPI_USE_ARRAY"/>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="hera">
    <DESC>NOAA hera system</DESC>
    <NODENAME_REGEX>hfe[0-9][0-9]\.hera</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi</MPILIBS>
    <PROJECT>nems</PROJECT>
    <SAVE_TIMING_DIR/>
    <CIME_OUTPUT_ROOT>$ENV{UFS_SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{UFS_INPUT}</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{UFS_INPUT}/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/scratch1/NCEPDEV/stmp2/CIME_UFSBASELINES</BASELINE_ROOT>
    <CCSM_CPRNC></CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>NCEP</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>80</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>40</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="num_tasks">-n $TOTALPES</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpi-serial">
      <executable></executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="sh">/apps/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/apps/lmod/lmod/init/csh</init_path>
      <init_path lang="python">/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="python">/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <modules compiler="intel">
        <command name="purge"/>
        <command name="load">intel/18.0.5.274</command>
      </modules>
      <modules mpilib="impi">
        <command name="load">netcdf/4.7.0</command>
        <command name="load">impi/2018.0.4</command>
	<command name="use">/scratch1/BMC/gmtb/software/modulefiles/intel-18.0.5.274/impi-2018.0.4</command>
	<command name="load">NCEPlibs/1.0.0alpha01</command>
      </modules>
      <modules>
        <command name="use">/scratch1/BMC/gmtb/software/modulefiles/generic</command>
        <command name="load">cmake/3.16.3</command>
      </modules>
    </module_system>
    <environment_variables comp_interface="nuopc">
      <env name="ESMF_RUNTIME_PROFILE">ON</env>
      <env name="ESMF_RUNTIME_PROFILE_OUTPUT">SUMMARY</env>
    </environment_variables>
  </machine>

  <machine MACH="jet">
    <DESC>NOAA JET system</DESC>
    <NODENAME_REGEX>fe\d?.fsl.noaa.gov</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi</MPILIBS>
    <PROJECT>wrfruc</PROJECT>
    <CIME_OUTPUT_ROOT>$ENV{UFS_SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{UFS_INPUT}/ufs_inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>${DIN_LOC_ROOT}/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>${CIME_OUTPUT_ROOT}/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{UFS_INPUT}/ufs_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/mnt/lfs1/projects/wrfruc/mhu/ufs/cime/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>NOAA</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>40</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>40</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
	<arg name="num_tasks">-n $TOTALPES</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpi-serial">
      <executable></executable>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/apps/lmod/lmod/init/perl</init_path> 
      <init_path lang="python">/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/apps/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/apps/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules compiler="intel">
        <command name="purge"/>
        <command name="load">intel/18.0.5.274</command>
      </modules>
      <modules mpilib="impi">
        <command name="load">impi/2018.4.274</command>
        <command name="load">pnetcdf/1.6.1</command>
        <command name="load">netcdf/4.7.0</command>
        <command name="use">/lfs3/projects/hfv3gfs/GMTB/modulefiles/intel-18.0.5.274/impi-2018.4.274</command>
        <command name="load">NCEPlibs/1.0.0beta02</command>
      </modules>
      <modules>
        <command name="use">/lfs3/projects/hfv3gfs/GMTB/modulefiles/generic</command>
        <command name="load">cmake/3.16.4</command>
      </modules>
    </module_system>
    <environment_variables comp_interface="nuopc">
      <env name="ESMF_RUNTIME_PROFILE">ON</env>
      <env name="ESMF_RUNTIME_PROFILE_OUTPUT">SUMMARY</env>
    </environment_variables>
  </machine>

  <machine MACH="linux">
    <DESC>
      Customize these fields as appropriate for your system,
      particularly changing MAX_TASKS_PER_NODE and MAX_MPITASKS_PER_NODE to the
      number of cores on your machine.
    </DESC>
    <NODENAME_REGEX> something.matching.your.machine.hostname </NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{UFS_SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{UFS_INPUT}/ufs-inputdata</DIN_LOC_ROOT>
    <DOUT_S_ROOT>$ENV{UFS_SCRATCH}/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{UFS_INPUT}/baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$CIMEROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>__YOUR_NAME_HERE__</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>8</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>8</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
	<arg name="anum_tasks"> -np {{ total_tasks }}</arg>
	<arg name="labelstdout">-prepend-rank</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
  </machine>

  <machine MACH="macos">
    <DESC>
      Customize these fields as appropriate for your system,
      particularly changing MAX_TASKS_PER_NODE and MAX_MPITASKS_PER_NODE to the
      number of cores on your machine.
    </DESC>
    <NODENAME_REGEX> something.matching.your.machine.hostname </NODENAME_REGEX>
    <OS>Darwin</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{UFS_SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{UFS_INPUT}/ufs-inputdata</DIN_LOC_ROOT>
    <DOUT_S_ROOT>$ENV{UFS_SCRATCH}/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{UFS_INPUT}/baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$CIMEROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>__YOUR_NAME_HERE__</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>8</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>8</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
	<arg name="anum_tasks"> -np {{ total_tasks }}</arg>
	<arg name="labelstdout">-prepend-rank</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
  </machine>

  <machine MACH="stampede2-skx">
    <DESC>Intel Xeon Platinum 8160 ("Skylake"),48 cores on two sockets (24 cores/socket) , batch system is SLURM</DESC>
    <NODENAME_REGEX>.*stampede2</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi</MPILIBS>
    <PROJECT>TG-ATM190009</PROJECT>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{UFS_INPUT}/ufs_inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>${DIN_LOC_ROOT}/lmwg</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{WORK}/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{WORK}/ufs_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/work/02503/edwardsj/CESM/cime/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>96</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>48</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="impi">
      <executable>ibrun</executable>
      <arguments>
	<arg name="ntasks"> -n {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mvapich2">
      <executable>ibrun</executable>
      <arguments>
	<arg name="ntasks"> -n {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/apps/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/opt/apps/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/opt/apps/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/opt/apps/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/opt/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/opt/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"></command>
        <command name="load">TACC</command>
        <command name="load">python/2.7.15</command>
        <command name="load">intel/18.0.2</command>
        <command name="load">cmake</command>
      </modules>
      <modules mpilib="mvapich2">
        <command name="load">mvapich2/2.3b</command>
        <command name="load">pnetcdf/1.11.0</command>
        <command name="load">netcdf/4.6.2</command>
      </modules>
      <modules mpilib="impi">
        <command name="rm">mvapich2</command>
        <command name="load">impi/18.0.2</command>
        <command name="load">pnetcdf/1.11.0</command>
        <command name="load">netcdf/4.6.2</command>
      </modules>
      <modules mpilib="mpi-serial">
        <command name="load">netcdf/4.6.2</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
     </environment_variables>
    <environment_variables comp_interface="nuopc">
      <env name="ESMF_RUNTIME_PROFILE">ON</env>
      <env name="ESMF_RUNTIME_PROFILE_OUTPUT">SUMMARY</env>
    </environment_variables>
    <environment_variables comp_interface="nuopc">
      <env name="NETCDF">$ENV{TACC_NETCDF_DIR}</env>
      <env name="NEMSIO_INC">$ENV{NCEPLIBS_DIR}/include</env>
      <env name="NEMSIO_LIB">$ENV{NCEPLIBS_DIR}/lib/libnemsio_v2.2.3.a</env>
      <env name="BACIO_LIB4">$ENV{NCEPLIBS_DIR}/lib/libbacio_v2.1.0_4.a</env>
      <env name="SP_LIBd">$ENV{NCEPLIBS_DIR}/lib/libsp_v2.0.2_d.a</env>
      <env name="W3EMC_LIBd">$ENV{NCEPLIBS_DIR}/lib/libw3emc_v2.2.0_d.a</env>
      <env name="W3NCO_LIBd">$ENV{NCEPLIBS_DIR}/lib/libw3nco_v2.0.6_d.a</env>
    </environment_variables>
  </machine>


  <default_run_suffix>
    <default_run_exe>${EXEROOT}/ufs.exe </default_run_exe>
    <default_run_misc_suffix> >> ufs.log.$LID 2>&amp;1 </default_run_misc_suffix>
  </default_run_suffix>

</config_machines>
