#!/usr/bin/env python

"""
Script to run CIME tests.

Runs single tests or test suites based on either the input list or the testname or based
on an xml testlist if the xml suboption is provided.

If this tool is missing any feature that you need, please notify jgfouca@sandia.gov.
"""
from Tools.standard_script_setup import *

import update_acme_tests
from CIME.system_test import SystemTest, RUN_PHASE
from CIME.utils import expect, convert_to_seconds, compute_total_time, convert_to_babylonian_time, run_cmd_no_fail
from CIME.XML.machines import Machines
from CIME.case import Case

import argparse, math, glob

logger = logging.getLogger(__name__)

###############################################################################
def parse_command_line(args, description):
###############################################################################
    parser = argparse.ArgumentParser(
        usage="""\n%s <TEST|SUITE> [<TEST|SUITE> ...] [--verbose]
OR
%s xml --category [CATEGORY] [--machine ...] [--compiler ...] [ --testlist ...]
OR
%s --help
OR
%s --test

\033[1mEXAMPLES:\033[0m
    \033[1;32m# Run single test \033[0m
    > %s <TESTNAME>

    \033[1;32m# Run test suite \033[0m
    > %s <SUITE>

    \033[1;32m# Run two tests \033[0m
    > %s <TESTNAME1> <TESTNAME2>

    \033[1;32m# Run two suites \033[0m
    > %s <SUITE1> <SUITE2>

    \033[1;32m# Run all tests in a suite except for one \033[0m
    > %s <SUITE> ^<TESTNAME>

    \033[1;32m# Run all tests in a suite except for tests that are in another suite \033[0m
    > %s <SUITE1> ^<SUITE2>

    \033[1;32m# Run all tests in the xml prealpha category and yellowstone machine \033[0m
    > %s --xml-machine yellowstone --xml-category prealpha

""" % ((os.path.basename(args[0]), ) * 11),

        description=description,

        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    CIME.utils.setup_standard_logging_options(parser)

    parser.add_argument("testargs", nargs="*",
                        help="Tests or test suites to run."
                        " Testnames expect in form CASE.GRID.COMPSET")

    parser.add_argument("--no-run", action="store_true",
                        help="Do not run generated tests")

    parser.add_argument("--no-build", action="store_true",
                        help="Do not build generated tests, implies --no-run")

    parser.add_argument("-u", "--use-existing", action="store_true",
                        help="Use pre-existing case directories. Requires test-id")

    parser.add_argument("--save-timing", action="store_true",
                        help="Enable archiving of performance data.")

    parser.add_argument("--no-batch", action="store_true",
                        help="Do not submit jobs to batch system, run locally."
                        " If false, will default to machine setting.")

    parser.add_argument("--single-submit", action="store_true",
                        help="Use a single interactive allocation to run all the tests. "
                        "Can drastically reduce queue waiting. Only makes sense on batch machines.")

    parser.add_argument("-r", "--test-root",
                        help="Where test cases will be created."
                        " Will default to scratch root XML machine file")

    parser.add_argument("--baseline-root",
                        help="Specifies an root directory for baseline"
                        "datasets used for Bit-for-bit generate/compare"
                        "testing.")

    parser.add_argument("--clean", action="store_true",
                        help="Specifies if tests should be cleaned after run. If set, "
                        "all object executables, and data files will"
                        " be removed after tests are run")

    parser.add_argument("-c", "--compare", const=True, nargs="?",
                        help="While testing, compare baselines,"
                        " optionally provide a compare directory ")

    parser.add_argument("-g", "--generate", const=True, nargs="?",
                        help="While testing, generate baselines,"
                        " optionally provide a generate directory")

    parser.add_argument("-b", "--baseline-name",
                        help="If comparing or generating baselines with default paths,"
                        " use this directory under baseline root. "
                        "Default will be current branch name. Do NOT add the compiler to"
                        " this argument that will be done for you.  If you provide directories"
                        " in the generate and or compare argument do not use this option.")

    parser.add_argument("--compiler",
                        help="Compiler to use to build cime.  Default will be the name in"
                        " the Testnames or the default defined for the machine.")

    parser.add_argument("-m", "--machine",
                        help="The machine for which to build tests, this machine must be defined"
                        " in the config_machines.xml file for the given model. "
                        "Default is to match the name of the machine in the test name or "
                        "the name of the machine this script is run on to the "
                        "NODENAME_REGEX field in config_machines.xml")

    parser.add_argument("-n", "--namelists-only", action="store_true",
                        help="Only perform namelist actions for tests")

    parser.add_argument("-p", "--project",
                        help="Specify a project id for the case (optional)."
                        "Used for accounting when on a batch system."
                        "The default is user-specified environment variable PROJECT")

    parser.add_argument("-t", "--test-id",
                        help="Specify an 'id' for the test. This is simply a"
                        "string that is appended to the end of a test name."
                        "If no testid is specified, then a time stamp will be"
                        "used.")

    parser.add_argument("-j", "--parallel-jobs", type=int, default=None,
                        help="Number of tasks create_test should perform simultaneously. Default "
                        "will be min(num_cores, num_tests).")

    parser.add_argument("--proc-pool", type=int, default=None,
                        help="The size of the processor pool that create_test can use. Default "
                        "is PES_PER_NODE + 25 percent.")

    parser.add_argument("--walltime", default=os.getenv("CIME_GLOBAL_WALLTIME"),
                        help="Set the wallclock limit for all tests in the suite. "
                        "Can use env var CIME_GLOBAL_WALLTIME to set this for all test.")

    parser.add_argument("--xml-machine",
                        help="Use this machine key in the lookup in testlist.xml")
    parser.add_argument("--xml-compiler",
                        help="Use this compiler key in the lookup in testlist.xml")
    parser.add_argument("--xml-category",
                        help="Use this category key in the lookup in testlist.xml")
    parser.add_argument("--xml-testlist",
                        help="Use this testlist to lookup tests")

    args = parser.parse_args(args[1:])

    CIME.utils.handle_standard_logging_options(args)

    # generate and compare flags may not point to the same directory
    if args.generate is not None:
        expect(not (args.generate == args.compare),
               "Cannot generate and compare baselines at the same time")
    expect(not (args.baseline_name is not None and (not args.compare and not args.generate)),
           "Provided baseline name but did not specify compare or generate")
    expect(not (args.namelists_only and not (args.generate or args.compare)),
           "Must provide either --compare or --generate with --namelists-only")

    if args.parallel_jobs is not None:
        expect(args.parallel_jobs > 0,
               "Invalid value for parallel_jobs: %d" % args.parallel_jobs)

    if args.xml_testlist is not None:
        expect(not (args.xml_machine is None and args.xml_compiler
                    is  None and args.xml_category is None),
               "If an xml-testlist is present at least one of --xml-machine, "
               "--xml-compiler, --xml-category must also be present")

    if args.use_existing:
        expect(args.test_id is not None, "Must provide test-id of pre-existing cases")

    if args.no_build:
        args.no_run = True

    # Namelist-only forces some other options:
    if args.namelists_only:
        args.no_build = True
        args.no_run   = True
        args.no_batch = True

    if args.single_submit:
        expect(not args.no_run, "Doesn't make sense to request single-submit if no-run is on")
        args.no_build = True
        args.no_run   = True
        args.no_batch = True

    if args.test_id is None:
        args.test_id = CIME.utils.get_utc_timestamp()

    return args.testargs, args.compiler, args.machine, args.no_run, args.no_build, args.no_batch,\
        args.test_root, args.baseline_root, args.clean, args.compare, args.generate, \
        args.baseline_name, args.namelists_only, args.project, args.test_id, args.parallel_jobs, \
        args.xml_machine, args.xml_compiler, args.xml_category, args.xml_testlist, args.walltime, \
        args.single_submit, args.proc_pool, args.use_existing, args.save_timing

###############################################################################
def single_submit_impl(machine_name, test_id, proc_pool, project, args, job_cost_map, wall_time, test_root):
###############################################################################
    mach = Machines(machine=machine_name)
    expect(mach.has_batch_system(), "Single submit does not make sense on non-batch machine '%s'" % mach.get_machine_name())

    machine_name = mach.get_machine_name()

    if project is None:
        project = CIME.utils.get_project(mach)
        if project is None:
            project = mach.get_value("PROJECT")

    #
    # Compute arg list for second call to create_test
    #
    new_args = list(args)
    new_args.remove("--single-submit")
    new_args.append("--no-batch")
    new_args.append("--use-existing")
    no_arg_is_a_test_id_arg = True
    no_arg_is_a_proc_pool_arg = True
    no_arg_is_a_machine_arg = True
    for arg in new_args:
        if arg == "-t" or arg.startswith("--test-id"):
            no_arg_is_a_test_id_arg = False
        elif arg.startswith("--proc-pool"):
            no_arg_is_a_proc_pool_arg = False
        elif arg == "-m" or arg.startswith("--machine"):
            no_arg_is_a_machine_arg = True

    if no_arg_is_a_test_id_arg:
        new_args.append("-t %s" % test_id)
    if no_arg_is_a_proc_pool_arg:
        new_args.append("--proc-pool %d" % proc_pool)
    if no_arg_is_a_machine_arg:
        new_args.append("-m %s" % machine_name)

    #
    # Resolve batch directives manually. There is currently no other way
    # to do this without making a Case object. Make a throwaway case object
    # to help us here.
    #
    testcase_dirs = glob.glob("%s/*%s*/TestStatus" % (test_root, test_id))
    expect(testcase_dirs, "No test case dirs found!?")
    first_case = os.path.abspath(os.path.dirname(testcase_dirs[0]))
    with Case(first_case, read_only=False) as case:
        env_batch = case.get_env("batch")

        directives = env_batch.get_batch_directives(case, "case.run", raw=True)
        submit_cmd  = env_batch.get_value("batch_submit", subgroup=None)
        submit_args = env_batch.get_submit_args(case, "case.run")

    tasks_per_node = int(mach.get_value("PES_PER_NODE"))
    num_nodes = int(math.ceil(float(proc_pool) / tasks_per_node))
    if wall_time is None:
        wall_time = compute_total_time(job_cost_map, proc_pool)
        wall_time_bab = convert_to_babylonian_time(int(wall_time))
    else:
        wall_time_bab = wall_time

    queue = env_batch.select_best_queue(proc_pool)
    wall_time_max_bab = env_batch.get_max_walltime(queue)
    if wall_time_max_bab is not None:
        wall_time_max = convert_to_seconds(wall_time_max_bab)
        if wall_time_max < wall_time:
            wall_time = wall_time_max
            wall_time_bab = convert_to_babylonian_time(wall_time)

    job_id = "create_test_single_submit_%s" % test_id
    directives = directives.replace("{{ job_id }}", job_id)
    directives = directives.replace("{{ num_nodes }}", str(num_nodes))
    directives = directives.replace("{{ tasks_per_node }}", str(tasks_per_node))
    directives = directives.replace("{{ ptile }}", str(tasks_per_node))
    directives = directives.replace("{{ totaltasks }}", str(tasks_per_node * num_nodes))

    directives = directives.replace("{{ output_error_path }}", "create_test_single_submit_%s.err" % test_id)
    directives = directives.replace("{{ job_wallclock_time }}", wall_time_bab)
    directives = directives.replace("{{ job_queue }}", queue)
    if project is not None:
        directives = directives.replace("{{ project }}", project)

    expect("{{" not in directives, "Could not resolve all items in directives:\n%s" % directives)

    #
    # Make simple submit script and submit
    #

    script = "#! /bin/bash\n"
    script += "\n%s" % directives
    script += "\n"
    script += "cd %s\n"%os.getcwd()
    script += "%s %s\n" % (__file__, " ".join(new_args))

    submit_cmd = "%s %s" % (submit_cmd, submit_args)
    logger.info("Script:\n%s" % script)

    run_cmd_no_fail(submit_cmd, input_str=script, arg_stdout=None, arg_stderr=None, verbose=True)

###############################################################################
def create_test(testargs, compiler, machine_name, no_run, no_build, no_batch, test_root,
                baseline_root, clean, compare, generate,
                baseline_name, namelists_only, project, test_id, parallel_jobs,
                xml_machine, xml_compiler, xml_category, xml_testlist, walltime,
                single_submit, proc_pool, use_existing, save_timing):
###############################################################################
    if testargs and machine_name is None and compiler is None:
        for test in testargs:
            testsplit = CIME.utils.parse_test_name(test)
            if testsplit[4] is not None:
                if machine_name is None:
                    machine_name = testsplit[4]
                else:
                    expect(machine_name == testsplit[4],
                           "ambiguity in machine, please use the --machine option")
            if testsplit[5] is not None:
                if compiler is None:
                    compiler = testsplit[5]
                else:
                    expect(compiler == testsplit[5],
                           "ambiguity in compiler, please use the --compiler option")

    impl = SystemTest(testargs,
                      no_run=no_run, no_build=no_build, no_batch=no_batch,
                      test_root=test_root, test_id=test_id,
                      baseline_root=baseline_root, baseline_name=baseline_name,
                      clean=clean, machine_name=machine_name, compiler=compiler,
                      compare=compare, generate=generate, namelists_only=namelists_only,
                      project=project, parallel_jobs=parallel_jobs,
                      xml_machine=xml_machine, xml_compiler=xml_compiler,
                      xml_category=xml_category, xml_testlist=xml_testlist, walltime=walltime,
                      proc_pool=proc_pool, use_existing=use_existing, save_timing=save_timing)

    success = impl.system_test()

    if single_submit:
        # Get real test root
        test_root = impl._test_root

        job_cost_map = {}
        largest_case = 0
        for test in impl._tests:
            test_dir = impl._get_test_dir(test)
            procs_needed = impl._get_procs_needed(test, RUN_PHASE)
            time_needed = convert_to_seconds(run_cmd_no_fail("./xmlquery JOB_WALLCLOCK_TIME -value", from_dir=test_dir))
            job_cost_map[test] = (procs_needed, time_needed)
            if procs_needed > largest_case:
                largest_case = procs_needed

        if proc_pool is None:
            # Based on size of created jobs, choose a reasonable proc_pool. May need to put
            # more thought into this.
            proc_pool = 2 * largest_case

        # Create submit script
        single_submit_impl(machine_name, test_id, proc_pool, project, sys.argv[1:], job_cost_map, walltime, test_root)

    return 0 if success else CIME.utils.TESTS_FAILED_ERR_CODE

###############################################################################
def _main_func(description):
###############################################################################
    if "--test" in sys.argv:
        CIME.utils.run_cmd_no_fail("python -m doctest %s/CIME/system_test.py -v" %
                                   CIME.utils.get_python_libs_root(), arg_stdout=None, arg_stderr=None)
        return

    testargs, compiler, machine_name, no_run, no_build, no_batch, test_root, baseline_root, clean, \
        compare, generate, baseline_name, namelists_only, project, test_id, parallel_jobs, \
        xml_machine, xml_compiler, xml_category, xml_testlist, walltime, single_submit, proc_pool, \
        use_existing, save_timing \
        = parse_command_line(sys.argv, description)

    sys.exit(create_test(testargs, compiler, machine_name, no_run, no_build, no_batch, test_root,
                         baseline_root, clean, compare, generate, baseline_name, namelists_only,
                         project, test_id, parallel_jobs, xml_machine, xml_compiler, xml_category,
                         xml_testlist, walltime, single_submit, proc_pool, use_existing, save_timing))

###############################################################################

if __name__ == "__main__":
    _main_func(__doc__)
